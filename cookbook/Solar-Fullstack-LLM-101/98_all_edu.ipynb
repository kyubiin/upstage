{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/UpstageAI/cookbook/blob/main/Solar-Fullstack-LLM-101/98_all_edu.ipynb\">\n",
    "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upstage Solar Full Stack LLM 101\n",
    "## Code to Understand!\n",
    "![Overview](./figures/overview.png)\n",
    "\n",
    "\n",
    "### Table of Contents\n",
    "* <b> Session 1. Hello Solar </b> : Obtain an Upstage API Key and use the upstage chat model. <br>\n",
    "    - 1-1 Interacting with the Solar-1-mini-chat Model\n",
    "    - 1-2 Using Few-Shot Examples in Chat Completions\n",
    "\n",
    "\n",
    "- <b> Session 2. Building LLM Applications with LangChain</b> :  Learn how to easily implement LLM chains using LangChain and understand the features of LLMs.<br>\n",
    "    - 2-1 Prompt Engineering\n",
    "    - 2-2 Hallucinations\n",
    "    - 2-3 Groundedness Check with LangChain and Upstage\n",
    "\n",
    "\n",
    "- <b> Session 3. What is RAG? </b>:  Understand the concept of RAG, load documents, and implement a RAG system.<br>\n",
    "    - 3-1 Layout Analysis\n",
    "    - 3-2 Retrieval Augmented Generation (RAG) for Question Answering\n",
    "    - 3-3 RAG Limitations <br>\n",
    "\n",
    "\n",
    "- <b> Session 4. Efficient Text Splitting and Indexing with LangChain </b>:  Efficiently build a RAG system by loading external documents, splitting them into smaller chunks, using embedding APIs to store them in a vectorspace, and retrieving them.<br>\n",
    "\n",
    "- <b> Session 5. Gradio </b>: Use Gradio and RAG techniques to process PDF documents and generate real-time, interactive responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (24.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: typer in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.12.3)\n",
      "Requirement already satisfied: click>=8.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from typer) (8.1.7)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from typer) (4.11.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from typer) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from typer) (13.7.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from rich>=10.11.0->typer) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/kyubin/Library/Python/3.10/lib/python/site-packages (from rich>=10.11.0->typer) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer) (0.1.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pip\n",
    "%pip install typer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: typer 0.12.3 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "zsh:1: no matches found: arize-phoenix[evals]\n"
     ]
    }
   ],
   "source": [
    "! pip3 install -qU guardrails-ai openai langchain_community langchain_experimental langchain-upstage sentence-transformers langchainhub langchain-chroma langchain matplotlib python-dotenv tavily-python ragas faiss-cpu tokenizers getpass4\n",
    "! pip3 install -q arize-phoenix[evals]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Session 1] HELLO SOLAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "<b> Introduction to Solar Framework </b>: Learn the basics of setting up the Solar LLM framework and running a simple \"Hello, World!\" example to understand its core functionality.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UPSTAGE_API_KEY\n",
    "To obtain your Upstage API key, follow these steps:\n",
    "\n",
    "1. Visit the Upstage AI console at <https://console.upstage.ai>.\n",
    "2. Sign up for an account if you don't already have one.\n",
    "3. Log in to your account.\n",
    "4. Navigate to the API key section.\n",
    "5. Generate your API key.\n",
    "6. Copy the key and save it securely.\n",
    "\n",
    "![Console](./figures/console.upstage.ai.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv\n",
    "# UPSTAGE_API_KEY from https://console.upstage.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title set API key\n",
    "import os\n",
    "import getpass\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from IPython import get_ipython\n",
    "\n",
    "if \"google.colab\" in str(get_ipython()):\n",
    "    # Running in Google Colab. Please set the UPSTAGE_API_KEY in the Colab Secrets\n",
    "    from google.colab import userdata\n",
    "    os.environ[\"UPSTAGE_API_KEY\"] = userdata.get(\"UPSTAGE_API_KEY\")\n",
    "else:\n",
    "    # Running locally. Please set the UPSTAGE_API_KEY in the .env file\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv()\n",
    "\n",
    "if \"UPSTAGE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"UPSTAGE_API_KEY\"] = getpass.getpass(\"Enter your Upstage API key: \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  1-1 Interacting with the Solar-1-mini-chat Model\n",
    "\n",
    "This Python code demonstrates how to use the OpenAI API to interact with the Solar-1-mini-chat model provided by Upstage AI.\n",
    "\n",
    "##### Steps\n",
    "\n",
    "1. Import necessary libraries: `os`, `openai`, and `pprint`.\n",
    "2. Set up the OpenAI client with the API key and base URL.\n",
    "3. Create a chat completion request using `client.chat.completions.create()`.\n",
    "   - Specify the model: \"solar-1-mini-chat\".\n",
    "   - Provide a list of messages, including the system message and user message.\n",
    "4. Handle the model's response:\n",
    "   - Print the entire response using `pprint()`.\n",
    "   - Print the content of the assistant's message using `response.choices[0].message.content`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='7d8bde3d-5973-4e96-a0d5-bdc1b34143ee', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Ah, Korea. A fascinating country with a rich history and a vibrant culture. I've always been intrigued by the blend of modernity and tradition in places like Seoul. The food, too, is something else - I've heard incredible things about Korean BBQ and kimchi. And let's not forget the beautiful landscapes, from the mountains to the coastline. It's definitely on my list of places to visit.\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1730456716, model='solar-mini-240612', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=94, prompt_tokens=26, total_tokens=120, completion_tokens_details=None, prompt_tokens_details=None))\n",
      "Message only:\n",
      "('Ah, Korea. A fascinating country with a rich history and a vibrant culture. '\n",
      " \"I've always been intrigued by the blend of modernity and tradition in places \"\n",
      " \"like Seoul. The food, too, is something else - I've heard incredible things \"\n",
      " \"about Korean BBQ and kimchi. And let's not forget the beautiful landscapes, \"\n",
      " \"from the mountains to the coastline. It's definitely on my list of places to \"\n",
      " 'visit.')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from pprint import pprint\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ[\"UPSTAGE_API_KEY\"], base_url=\"https://api.upstage.ai/v1/solar\"\n",
    ")\n",
    "chat_result = client.chat.completions.create(\n",
    "    model=\"solar-1-mini-chat\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What about Korea?\"},\n",
    "    ],\n",
    ")\n",
    "pprint(chat_result)\n",
    "print(\"Message only:\")\n",
    "pprint(chat_result.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-2 Using Few-Shot Examples in Chat Completions\n",
    "\n",
    "This Python code demonstrates how to use few-shot examples in the OpenAI Chat Completions API to provide context and guide the model's responses.\n",
    "\n",
    "##### Steps\n",
    "\n",
    "1. Set up the OpenAI client with the API key and base URL.\n",
    "2. Create a chat completion request using `client.chat.completions.create()`.\n",
    "   - Specify the model: \"solar-1-mini-chat\".\n",
    "   - Provide a list of messages, including:\n",
    "     - System message: Defines the assistant's role.\n",
    "     - Few-shot examples: Provide context and desired behavior.\n",
    "     - User input: The actual user query.\n",
    "3. Handle the model's response:\n",
    "   - Print the entire response using `pprint()`.\n",
    "   - Print the content of the assistant's message using `response.choices[0].message.content`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='134f13fe-d536-4db2-b07f-bf31ac62494f', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Oh, Korea has two separate countries, North Korea and South Korea. The capital of South Korea, which is the one most people are referring to when they say \"Korea\", is Seoul.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1730456717, model='solar-mini-240612', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=41, prompt_tokens=55, total_tokens=96, completion_tokens_details=None, prompt_tokens_details=None))\n",
      "Message only:\n",
      "('Oh, Korea has two separate countries, North Korea and South Korea. The '\n",
      " 'capital of South Korea, which is the one most people are referring to when '\n",
      " 'they say \"Korea\", is Seoul.')\n"
     ]
    }
   ],
   "source": [
    "# few shots: examples or history\n",
    "chat_result = client.chat.completions.create(\n",
    "    model=\"solar-1-mini-chat\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        # examples\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"I know of it. It's Paris!!\",\n",
    "        },\n",
    "        # user input\n",
    "        {\"role\": \"user\", \"content\": \"What about Korea?\"},\n",
    "    ],\n",
    ")\n",
    "pprint(chat_result)\n",
    "print(\"Message only:\")\n",
    "pprint(chat_result.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Session 2] Building LLM Applications with LangChain\n",
    "\n",
    "This Python code demonstrates how to use the LangChain library to build applications with Large Language Models (LLMs). It covers the basic steps of defining an LLM, creating a chat prompt, defining a chain, and invoking the chain.\n",
    "\n",
    "#### Steps\n",
    "\n",
    "1. Define your favorite LLM:\n",
    "   - Import the `ChatUpstage` class from `langchain_upstage`.\n",
    "   - Create an instance of `ChatUpstage` and assign it to the variable `llm`.\n",
    "\n",
    "2. Define a chat prompt:\n",
    "   - Import the `ChatPromptTemplate` class from `langchain_core.prompts`.\n",
    "   - Create a `ChatPromptTemplate` instance using the `from_messages()` method.\n",
    "   - Provide a list of messages, including system messages, example conversations, and user input.\n",
    "\n",
    "3. Define a chain:\n",
    "   - Import the `StrOutputParser` class from `langchain_core.output_parsers`.\n",
    "   - Create a chain by combining the `rag_with_history_prompt`, `llm`, and `StrOutputParser()` using the pipe (`|`) operator.\n",
    "\n",
    "4. Invoke the chain:\n",
    "   - Call the `invoke()` method on the `chain` object, passing an empty dictionary (`{}`) as the input.\n",
    "   - Print the response obtained from the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! I'm an artificial intelligence and don't have feelings or emotions, so I can't experience emotions like humans do. However, I'm here to assist you with any questions or tasks you may have. How can I help you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 53, 'prompt_tokens': 16, 'total_tokens': 69, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'solar-mini-240612', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-73dc463b-e2fa-486f-b450-b75f5d58f53e-0', usage_metadata={'input_tokens': 16, 'output_tokens': 53, 'total_tokens': 69, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick hello world\n",
    "from langchain_upstage import ChatUpstage\n",
    "\n",
    "llm = ChatUpstage()\n",
    "llm.invoke(\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, you mean South Korea? The capital of South Korea is Seoul.\n"
     ]
    }
   ],
   "source": [
    "# langchain, 1. llm define, 2. prompt define, 3. chain, 4. chain.invoke\n",
    "\n",
    "# 1. define your favorate llm, solar\n",
    "from langchain_upstage import ChatUpstage\n",
    "\n",
    "llm = ChatUpstage()\n",
    "\n",
    "# 2. define chat prompt\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "rag_with_history_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\", \"What is the capital of France?\"),\n",
    "        (\"ai\", \"I know of it. It's Paris!!\"),\n",
    "        (\"human\", \"What about Korea?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 3. define chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = rag_with_history_prompt | llm | StrOutputParser()\n",
    "\n",
    "# 4. invoke the chain\n",
    "c_result = chain.invoke({})\n",
    "print(c_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1 Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameterized Prompt Templates in LangChain\n",
    "\n",
    "##### Overview\n",
    "\n",
    "- Prompt templates allow for reusable and modular prompts\n",
    "- They improve maintainability compared to using raw prompt strings\n",
    "- {country} value can be set from outside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ah, good question! The capital of North Korea is Pyongyang and the capital of South Korea is Seoul.\n",
      "---\n",
      "Oh, I didn't know that! But I'll find out for you. The capital of Japan is Tokyo.\n"
     ]
    }
   ],
   "source": [
    "# parameterized prompt template\n",
    "rag_with_history_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\", \"What is the capital of France?\"),\n",
    "        (\"ai\", \"I know of it. It's Paris!!\"),\n",
    "        (\"human\", \"What about {country}?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = rag_with_history_prompt | llm | StrOutputParser()\n",
    "\n",
    "# 4. invoke chain with param\n",
    "print(chain.invoke({\"country\": \"Korea\"}))\n",
    "print(\"---\")\n",
    "print(chain.invoke({\"country\": \"Japan\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leveraging Message History in LangChain Prompts\n",
    "\n",
    "- LangChain provides powerful tools for managing conversation history\n",
    "- `MessagesPlaceholder` allows for dynamic inclusion of message history\n",
    "- `HumanMessage` and `AIMessage` classes represent individual messages\n",
    "- Combining message history with user input enables context-aware responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Korea is actually divided into two countries: North Korea and South Korea. The capital of South Korea is Seoul, while the capital of North Korea is Pyongyang.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# More general chat\n",
    "rag_with_history_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "history = [\n",
    "    HumanMessage(\"What is the capital of France?\"),\n",
    "    AIMessage(\"It's Paris!!\"),\n",
    "]\n",
    "\n",
    "chain = rag_with_history_prompt | llm | StrOutputParser()\n",
    "chain_result = chain.invoke({\"history\": history, \"input\": \"What about Korea?\"})\n",
    "print(chain_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chain of Thought Prompting\n",
    "\n",
    "![CoT](figures/cot.webp)\n",
    "\n",
    "from https://arxiv.org/abs/2201.11903"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The cafeteria started with 23 apples. They used 20 of them, so they had 23 - 20 = 3 apples left. Then they bought 6 more apples, so they added those to the remaining 3 apples. 3 + 6 = 9 apples. Therefore, the cafeteria has 9 apples.'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Q: The cafeteria had 23 apples. \n",
    "If they used 20 to make lunch and bought 6 more, \n",
    "how many apples do they have?\n",
    "\n",
    "A: the answer is\n",
    "\"\"\"\n",
    ")\n",
    "chain = prompt_template | llm | StrOutputParser()\n",
    "chain.invoke({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The cafeteria started with 23 apples. They used 20, so they had 3 left. Then they bought 6 more, so they had 9. The answer is 9.'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
    "\n",
    "A: Roger started with 5 balls. 2 cans of 3 tennis balls\n",
    "each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
    "\n",
    "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\"\"\"\n",
    ")\n",
    "chain = prompt_template | llm | StrOutputParser()\n",
    "chain.invoke({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learn more advanced techniques by reading blog posts on Prompt Engineering!\n",
    "\n",
    "\n",
    "1. [[Prompt Engineering - Part 1] Maximizing the Use of LLM with Prompt Design](https://www.upstage.ai/feed/insight/prompt-engineering-guide-maximizing-the-use-of-llm-with-prompt-design)\n",
    "2. [[Prompt Engineering - Part 2] The Essence of Prompt Engineering: A Comprehensive Guide to Maximizing LLM Usage](https://www.upstage.ai/feed/insight/prompt-engineering-guide-to-maximizing-llm-usage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2 Hallucinations\n",
    "\n",
    "<b> Understanding Model Hallucinations </b> : Discover how to identify, understand, and mitigate hallucinations to ensure accurate and reliable model outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Hallucination](./figures/hallucination.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Upstage DUS (Design for Understanding and Success) is a technique used in the field of user experience (UX) design to create a comprehensive understanding of a product or service from the user\\'s perspective. The technique involves creating a visual representation of the user\\'s journey, or \"journey map,\" which shows the user\\'s interactions with the product or service over time.\\n\\nThe Upstage DUS technique is based on the idea that understanding the user\\'s perspective is key to creating a successful product or service. By creating a journey map, designers can identify pain points and areas for improvement in the user experience, and make informed decisions about how to design the product or service to better meet the user\\'s needs.\\n\\nThe Upstage DUS technique is a valuable tool for UX designers, as it helps them to understand the user\\'s perspective and create a product or service that is both useful and enjoyable to use. It is a useful technique for a variety of design projects, including websites, mobile apps, and physical products.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 215, 'prompt_tokens': 18, 'total_tokens': 233, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'solar-mini-240612', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-b5feac35-da18-4a95-b34e-93739aa01e69-0', usage_metadata={'input_tokens': 18, 'output_tokens': 215, 'total_tokens': 233, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cannot say \"I don't know\" :-)\n",
    "# Because it is trained to complete the sentence and try to answer the question\n",
    "llm.invoke(\"What is Upstage DUS technique?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next Token Prediction\n",
    "They are designed to generate the next words. It's also very difficult to know what we don't know.\n",
    "\n",
    "![image](https://jalammar.github.io/images/xlnet/gpt-2-autoregression-2.gif)\n",
    "\n",
    "Image from https://jalammar.github.io/illustrated-gpt2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Can We Mitigate Hallucinations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3 Groundedness Check with LangChain and Upstage\n",
    "![Groundedness](./figures/gc.png)\n",
    "\n",
    "[Groundedness Check](https://developers.upstage.ai/docs/apis/groundedness-check)\n",
    "\n",
    "#### High-Level Overview\n",
    "\n",
    "The provided code demonstrates how to perform a groundedness check using the LangChain library and the Upstage model. The groundedness check is a process of verifying whether the generated response is grounded in the given context. This is an important step in ensuring the quality and relevance of the generated output.\n",
    "\n",
    "The code uses the `UpstageGroundednessCheck` class from the `langchain_upstage` module to perform the groundedness check. It takes the context (a string of unique documents) and the generated response as input, and returns a verdict indicating whether the response is grounded or not.\n",
    "\n",
    "#### Detailed Explanation\n",
    "\n",
    "1. The code starts by importing the necessary module:\n",
    "   - `UpstageGroundednessCheck` from `langchain_upstage`: This class is used to perform the groundedness check.\n",
    "   \n",
    "\n",
    "2. An instance of the `UpstageGroundednessCheck` class is created and assigned to the variable `groundedness_check`.\n",
    "\n",
    "3. The input for the groundedness check is prepared by creating a dictionary called `request_input`:\n",
    "   - The `\"context\"` key is assigned the value of `str(unique_docs)`, which represents the unique documents as a string.\n",
    "   - The `\"answer\"` key is assigned the value of `response`, which represents the generated response.\n",
    "   \n",
    "\n",
    "4. The `invoke` method of the `groundedness_check` instance is called with the `request_input` as an argument. This method performs the groundedness check and returns the verdict.\n",
    "\n",
    "5. The verdict is stored in the `gc_result` variable and printed to the console using `print(gc_result)`.\n",
    "\n",
    "6. The code then checks if the `gc_result` starts with the word \"grounded\" (case-insensitive):\n",
    "   - If the response starts with \"grounded\", it means the groundedness check has passed, and the message \"✅ Groundedness check passed\" is printed.\n",
    "   - If the response does not start with \"grounded\", it means the groundedness check has failed, and the message \"❌ Groundedness check failed\" is printed.\n",
    "\n",
    "\n",
    "The provided code demonstrates a simple yet effective way to perform a groundedness check using LangChain and Upstage. By verifying whether the generated response is grounded in the given context, it helps ensure the quality and relevance of the output.\n",
    "\n",
    "Groundedness checks are an important step in building reliable and trustworthy language models and conversational agents. They help prevent the generation of irrelevant, inconsistent, or factually incorrect responses.\n",
    "\n",
    "By using the `UpstageGroundednessCheck` class from LangChain, developers can easily integrate groundedness checks into their language model pipelines and improve the overall performance of their systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grounded\n",
      "✅ Groundedness check passed\n"
     ]
    }
   ],
   "source": [
    "# GC\n",
    "from langchain_upstage import UpstageGroundednessCheck\n",
    "\n",
    "groundedness_check = UpstageGroundednessCheck()\n",
    "\n",
    "context = \"DUS is a new approach developed by Upstage to improve the search quality.\"\n",
    "answer = \"DUS is developed by Upstage.\"\n",
    "\n",
    "request_input = {\n",
    "    \"context\": context,\n",
    "    \"answer\": answer,\n",
    "}\n",
    "gc_result = groundedness_check.invoke(request_input)\n",
    "\n",
    "print(gc_result)\n",
    "if gc_result.lower().startswith(\"grounded\"):\n",
    "    print(\"✅ Groundedness check passed\")\n",
    "else:\n",
    "    print(\"❌ Groundedness check failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Groundedness check failed\n"
     ]
    }
   ],
   "source": [
    "context = \"DUS is a new approach developed by Upstage to improve the search quality.\"\n",
    "answer = \"DUS is developed by Google.\"\n",
    "\n",
    "request_input = {\n",
    "    \"context\": context,\n",
    "    \"answer\": answer,\n",
    "}\n",
    "gc_result = groundedness_check.invoke(request_input)\n",
    "\n",
    "if gc_result.lower().startswith(\"grounded\"):\n",
    "    print(\"✅ Groundedness check passed\")\n",
    "else:\n",
    "    print(\"❌ Groundedness check failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Session 3] What is RAG?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide context and allow the language model to respond within that context only.\n",
    "\n",
    "![Overview](./figures/rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1 Layout Analysis\n",
    "\n",
    "Leveraging Layout Analyzer and LangChain for Efficient Text Splitting and Vectorization\n",
    "\n",
    "- Upstage Layout Analyzer extracts layouts, tables, and figures from any document\n",
    "- LangChain provides powerful tools for text splitting and vectorization\n",
    "\n",
    "![Layout Analyzer](./figures/la.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_upstage import (\n",
    "    UpstageLayoutAnalysisLoader,\n",
    "    UpstageGroundednessCheck,\n",
    "    ChatUpstage,\n",
    "    UpstageEmbeddings,\n",
    ")\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "layzer = UpstageLayoutAnalysisLoader(\"pdfs/solar_paper.pdf\", output_type=\"html\")\n",
    "# For improved memory efficiency, consider using the lazy_load method to load documents page by page.\n",
    "docs = layzer.load()  # or layzer.lazy_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"<h1 id='0' style='font-size:20px'>SOLAR 10.7B: Scaling Large Language Models \"\n",
      " 'with Simple yet Effecti')\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    pprint(doc.page_content[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1 id='0' style='font-size:20px'>SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective<br>Depth Up-Scaling</h1><br><p id='1' data-category='paragraph' style='font-size:18px'>Dahyun Kim∗, Chanjun Park∗†, Sanghoon Kim∗†, Wonsung Lee∗†, Wonho Song∗<br>Yunsu Kim∗, Hyeonwoo Kim∗, Yungi Kim, Hyeonju Lee, Jihoo Kim<br>Changbae Ahn, Seonghoon Yang, Sukyung Lee, Hyunbyung Park, Gyoungjin Gim<br>Mikyoung Cha, Hwalsuk Lee†, Sunghun Kim†</p><p id='2' data-category='paragraph' style='font-size:18px'>Upstage AI, South Korea</p><br><p id='3' data-category='paragraph' style='font-size:14px'>{kdahyun, chanjun.park, limerobot, wonsung.lee, hwalsuk.lee, hunkim}@upstage.ai</p><br><p id='4' data-category='paragraph' style='font-size:18px'>Abstract</p><br><p id='5' data-category='paragraph' style='font-size:14px'>We introduce SOLAR 10.7B, a large language<br>model (LLM) with 10.7 billion parameters,<br>demonstrating superior performance in various<br>natural language processing (NLP) tasks. "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(docs[0].page_content[:1000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2 Retrieval Augmented Generation (RAG) for Question Answering\n",
    "\n",
    "- RAG combines retrieval and generation to enhance LLM performance on specific tasks\n",
    "- Relevant context is retrieved from external data sources and added to the prompt\n",
    "- The augmented prompt is then passed to the LLM for generating a response\n",
    "- RAG is particularly useful for question answering on custom datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESPONSE1\n",
      " Based on the provided context, I can provide a performance comparison among the merge candidates.\n",
      "\n",
      "According to Table 6, there are two merge candidates, named 'Cand. 1' and 'Cand. 2', which are trained using the same setting as 'DPO v2' and 'DPO v3', respectively, but with different hyper-parameters to maximize each model's respective strengths.\n",
      "\n",
      "The table shows the performance of these two candidates on various tasks, including H6 (average of six tasks), ARC, HellaSwag, MMLU, TruthfulQA, Winogrande, and GSM8K.\n",
      "\n",
      "From the table, we can see that 'Cand. 1' has high GSM8K scores but relatively low scores for the other tasks, whereas 'Cand. 2' has low scores for GSM8K but high scores for the other tasks.\n",
      "\n",
      "This indicates that 'Cand. 1' is more specialized in the GSM8K task, while 'Cand. 2' is more well-rounded in performance across various tasks.\n",
      "\n",
      "The choice between these two candidates would depend on the specific use case and the importance given to each task. If the primary focus is on GSM8K performance, 'Cand. 1' might be the better choice. If a more balanced performance across tasks is desired, 'Cand. 2' might be the better choice.\n",
      "\n",
      "In the next step, these two candidates are merged using different methods, and the results are ablated in Table 7. The merge methods include Average (a, b) with different weightings and SLERP. The ablation studies show that the different merge methods have little effect on the H6 scores and the scores for the individual tasks also do not differ by much.\n"
     ]
    }
   ],
   "source": [
    "# More general chat\n",
    "rag_with_history_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question considering the history of the conversation. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "---\n",
    "CONTEXT:\n",
    "{context}\n",
    "         \"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "history = []\n",
    "\n",
    "chain = rag_with_history_prompt | llm | StrOutputParser()\n",
    "query1 = \"Performance comparison amongst the merge candidate\"\n",
    "response1 = chain.invoke({\"history\": history, \"context\": docs, \"input\": query1})\n",
    "print(\"RESPONSE1\\n\", response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESPONSE2\n",
      " The ablation studies in the context provided focus on the different training datasets used for instruction tuning and alignment tuning, as well as the different SFT base models used for the alignment tuning stage.\n",
      "\n",
      "**Ablation on the training datasets for instruction tuning:**\n",
      "\n",
      "In Table 3, the authors present ablation studies using different training datasets for the instruction tuning stage. The ablated models are prefixed with SFT for supervised fine-tuning. 'SFT v1' only uses the Alpaca-GPT4 dataset, whereas 'SFT v2' also uses the OpenOrca dataset. 'SFT v3' uses the Synth. Math-Instruct dataset along with the datasets used in 'SFT v2'. Similarly, 'SFT v4' uses the Synth. Math-Instruct dataset along with the datasets used in 'SFT v1'.\n",
      "\n",
      "The table shows the performance of these ablated models on various tasks, including H6 (average of six tasks), ARC, HellaSwag, MMLU, TruthfulQA, Winogrande, and GSM8K.\n",
      "\n",
      "From the table, we can see that adding the OpenOrca dataset to train 'SFT v2' resulted in little change from 'SFT v1', but the task scores varied more. However, adding the Synth. Math-Instruct dataset to train 'SFT v3' and 'SFT v4' boosted GSM8K scores and achieved comparable scores for the other tasks.\n",
      "\n",
      "The authors conclude that adding the Synth. Math-Instruct dataset is helpful for improving the model's performance, especially in the GSM8K task.\n",
      "\n",
      "**Ablation on the training datasets for alignment tuning:**\n",
      "\n",
      "In Table 4, the authors present ablation studies on the different alignment datasets used during the direct preference optimization (DPO) stage. They use 'SFT v3' as the SFT base model for DPO. 'DPO v1' only uses the Ultrafeedback Clean dataset while 'DPO v2' also used the Synth. Math-Alignment dataset.\n",
      "\n",
      "The table shows the performance of these ablated models on various tasks, including H6 (average of six tasks), ARC, HellaSwag, MMLU, TruthfulQA, Winogrande, and GSM8K.\n",
      "\n",
      "From the table, we can see that using Ultrafeedback Clean and Synth. Math-Alignment impacted the model's performance. 'DPO v1' achieved 73.06 in H6, which is a substantial boost from the SFT base model score of 70.03. However, the score for GSM8K is 58.83, which is lower than the SFT base model score of 64.14. Adding Synth. Math-Alignment to train 'DPO v2', we see that the GSM8k score improves to 60.27, which is lower than the SFT base model but still higher than 'DPO v1'. Other task scores are also not negatively impacted by adding Synth. Math-Alignment.\n",
      "\n",
      "The authors conclude that adding Synth. Math-Alignment is beneficial for improving the model's performance, especially in the GSM8K task.\n",
      "\n",
      "**Ablation on the SFT base models for alignment tuning:**\n",
      "\n",
      "In Table 5, the authors ablate on the different SFT base models used for the alignment tuning stage. They use Ultrafeedback Clean and Synth. Math-Alignment datasets for this ablation. Each of the ablated models is trained as follows. 'DPO v2' uses 'SFT v3' as the base SFT model, while 'DPO v3' uses 'SFT v3+v4' as the SFT base model instead.\n",
      "\n",
      "The table shows the performance of these ablated models on various tasks, including H6 (average of six tasks), ARC, HellaSwag, MMLU, TruthfulQA, Winogrande, and GSM8K.\n",
      "\n",
      "From the table, we can see that 'DPO v3' has higher scores on all tasks compared to 'DPO v2', and the gap is especially large for ARC (+1.45) and GSM8K (+2.43). Surprisingly, the two models perform similarly in terms of H6. A closer look at the scores for the individual tasks shows only a small margin in the GSM8K scores, and other task scores show little difference.\n",
      "\n",
      "The authors conclude that the performance gaps in certain tasks in the SFT base models do not always carry over to the alignment-tuned models.\n",
      "\n",
      "In summary, the ablation studies show that adding the Synth. Math-Instruct dataset is helpful for improving the model's performance, especially in the GSM8K task. The choice of SFT base model for alignment tuning can impact the model's performance, but the differences are not always consistent across all tasks.\n"
     ]
    }
   ],
   "source": [
    "history = [HumanMessage(query1), AIMessage(response1)]\n",
    "query2 = \"How about Ablation studies?\"\n",
    "response2 = chain.invoke({\"history\": history, \"context\": docs, \"input\": query2})\n",
    "print(\"RESPONSE2\\n\", response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3  RAG Limitations\n",
    "- LLM does not have long enough context length\n",
    "- Sending long, irrelevant info is inefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load something big\n",
    "layzer = UpstageLayoutAnalysisLoader(\n",
    "    \"pdfs/kim-tse-2008.pdf\", output_type=\"html\", use_ocr=True\n",
    ")\n",
    "# For improved memory efficiency, consider using the lazy_load method to load documents page by page.\n",
    "docs = layzer.load()  # or layzer.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 32768 tokens. However, your messages resulted in 36624 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    }
   ],
   "source": [
    "# More general chat\n",
    "rag_with_history_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question considering the history of the conversation. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "---\n",
    "CONTEXT:\n",
    "{context}\n",
    "         \"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = rag_with_history_prompt | llm | StrOutputParser\n",
    "()\n",
    "query1 = \"What is bug classification?\"\n",
    "\n",
    "try:\n",
    "    response1 = chain.invoke({\"history\": history, \"context\": docs, \"input\": query1})\n",
    "    print(response1)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107727\n"
     ]
    }
   ],
   "source": [
    "print(len(docs[0].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer.from_pretrained(\"upstage/solar-1-mini-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded input: ['<|startoftext|>', '▁Nice', '▁to', '▁meet', '▁you', '.', '▁I', '▁am', '▁Solar', '▁LL', 'M', ',', '▁a', '▁large', '▁language', '▁model', '▁developed', '▁by', '▁Up', 'stage', '.', '▁If', '▁you', '▁have', '▁any', '▁questions', ',', '▁please', '▁feel', '▁free', '▁to', '▁ask', '.']\n",
      "Number of tokens: 33\n"
     ]
    }
   ],
   "source": [
    "text = \"Nice to meet you. I am Solar LLM, a large language model developed by Upstage. If you have any questions, please feel free to ask.\"\n",
    "\n",
    "enc = tokenizer.encode(text)\n",
    "print(\"Encoded input:\", enc.tokens)\n",
    "\n",
    "number_of_tokens = len(enc.tokens)\n",
    "print(\"Number of tokens:\", number_of_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded input: ['<|startoftext|>', '▁만나', '서', '▁반가', '워', '요', '.', '▁저는', '▁Up', 'stage', '에서', '▁개발한', '▁대규모', '▁언어', '▁모델', '인', '▁Solar', '▁LL', 'M', '▁입니다', '.', '▁궁금한', '▁것이', '▁있으', '시면', '▁무엇이', '든', '▁물어', '보세요', '.']\n",
      "Number of tokens: 30\n"
     ]
    }
   ],
   "source": [
    "text = \"만나서 반가워요. 저는 Upstage에서 개발한 대규모 언어 모델인 Solar LLM 입니다. 궁금한 것이 있으시면 무엇이든 물어보세요.\"\n",
    "enc = tokenizer.encode(text)\n",
    "print(\"Encoded input:\", enc.tokens)\n",
    "\n",
    "number_of_tokens = len(enc.tokens)\n",
    "print(\"Number of tokens:\", number_of_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_of_tokens(text):\n",
    "    return len(tokenizer.encode(text).tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENG 33\n",
      "KOR 30\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"ENG\",\n",
    "    num_of_tokens(\n",
    "        \"Nice to meet you. I am Solar LLM, a large language model developed by Upstage. If you have any questions, please feel free to ask.\"\n",
    "    ),\n",
    ")\n",
    "print(\n",
    "    \"KOR\",\n",
    "    num_of_tokens(\n",
    "        \"만나서 반가워요. 저는 Upstage에서 개발한 대규모 언어 모델인 Solar LLM 입니다. 궁금한 것이 있으시면 무엇이든 물어보세요.\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String length 107727\n",
      "Number of tokens 35128\n"
     ]
    }
   ],
   "source": [
    "# Recall\n",
    "# Let's load something big\n",
    "# layzer = UpstageLayoutAnalysisLoader(\"pdfs/kim-tse-2008.pdf\", output_type=\"html\")\n",
    "# For improved memory efficiency, consider using the lazy_load method to load documents page by page.\n",
    "# docs = layzer.load()  # or layzer.lazy_load()\n",
    "print(\"String length\", len(docs[0].page_content))\n",
    "print(\"Number of tokens\", num_of_tokens(docs[0].page_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Session 4] Efficient Text Splitting and Indexing with LangChain\n",
    "\n",
    "\n",
    "### Steps\n",
    "<b> 1. Load Documents </b>\n",
    "\n",
    "The first step is to load the source documents that will be used to augment the language model's knowledge\n",
    "This could be done by reading files from disk, pulling from a database, scraping web pages, etc.\n",
    "The goal is to get the raw text content into a format that can be further processed\n",
    "\n",
    "<b>2. Chunking/Splitting</b>\n",
    "\n",
    "* Long documents need to be broken down into smaller chunks that are a manageable size for embedding and retrieval\n",
    "Common approaches include:\n",
    "  * Fixed-size chunking - split text into equal sized chunks based on character or token count \n",
    "  * Semantic chunking - split based on semantic boundaries like sentences, paragraphs, or sections\n",
    "  * Hierarchical chunking - create chunks at multiple levels of granularity\n",
    "The ideal chunk size depends on the embedding model, retrieval use case, and downstream task\n",
    "\n",
    "<b>3. Embedding & Indexing</b>\n",
    "\n",
    "* The text chunks are converted to vector embeddings using a model like Upstage embeddings\n",
    "* The embeddings are indexed and stored in a vector database to enable efficient similarity search \n",
    "* Metadata about the source chunks can also be stored alongside the embeddings\n",
    "\n",
    "<b>4. Retrieval</b>\n",
    "\n",
    "* At query time, the user's question is itself embedded as a query vector\n",
    "* The query embedding is used to find the most similar document chunks in the vector index \n",
    "* Top-k most relevant chunks are retrieved and can be used to augment the prompt sent to the language model to generate an answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RecursiveCharacterTextSplitter\n",
    "\n",
    " `RecursiveCharacterTextSplitter` class is designed to be recursively split so that semantically related pieces remain together. <br>\n",
    " During this process, a list of delimiter characters `(['\\n\\n', '\\n', ' ', ''])` is used sequentially to partition the text. \n",
    "- This splitting continues until the resulting chunks are smaller than the specified `chunk_size`. \n",
    "- The `chunk_overlap` parameter defines the number of characters that should overlap between the divided text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG 1. load doc (done), 2. chunking, splits, 3. embeding - indexing, 4. retrieve\n",
    "\n",
    "# layzer = UpstageLayoutAnalysisLoader(\"pdfs/kim-tse-2008.pdf\", output_type=\"html\")\n",
    "# # For improved memory efficiency, consider using the lazy_load method to load documents page by page.\n",
    "# docs = layzer.load()  # or layzer.lazy_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits: 128\n"
     ]
    }
   ],
   "source": [
    "# 2. Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "print(\"Splits:\", len(splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "split_lengths = [len(split.page_content) for split in splits]\n",
    "\n",
    "# Create a bar graph\n",
    "plt.bar(range(len(split_lengths)), split_lengths)\n",
    "plt.title(\"RecursiveCharacterTextSplitter\")\n",
    "plt.xlabel(\"Split Index\")\n",
    "plt.ylabel(\"Split Content Length\")\n",
    "plt.xticks(range(len(split_lengths)), [])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.36 s, sys: 785 ms, total: 3.15 s\n",
      "Wall time: 22.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 3. Embed & indexing\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits, embedding=UpstageEmbeddings(model=\"solar-embedding-1-large\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "<h1 id='2' style='font-size:22px'>Classifying Software Changes:<br>Clean or Buggy?</h1><br><p id='3'\n"
     ]
    }
   ],
   "source": [
    "# 4. retrive\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "result_docs = retriever.invoke(\"What is Bug Classification?\")\n",
    "print(len(result_docs))\n",
    "print(result_docs[0].page_content[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SemanticChunker\n",
    "\n",
    "SemanticChunker is an experimental feature in LangChain that serves to split text into semantically similar chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Overview](./figures/semantic_chunker.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-2. SemanticChunker Split\n",
    "from langchain_community.utils.math import cosine_similarity\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "\n",
    "def semantic_chunker(\n",
    "    docs,\n",
    "    min_chunk_size=100,\n",
    "    chunk_overlap=10,\n",
    "    max_chunk_size=1000,\n",
    "    merge_threshold=0.7,\n",
    "    embeddings=UpstageEmbeddings(model=\"solar-embedding-1-large\"),\n",
    "):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=min_chunk_size, chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    init_splits = text_splitter.split_documents(docs)\n",
    "    splits = []\n",
    "\n",
    "    base_split_text = None\n",
    "    base_split_emb = None\n",
    "    for split in init_splits:\n",
    "        if base_split_text is None:\n",
    "            base_split_text = split.page_content\n",
    "            base_split_emb = embeddings.embed_documents([base_split_text])[0]\n",
    "            continue\n",
    "\n",
    "        split_emb = embeddings.embed_documents([split.page_content])[0]\n",
    "        distance = cosine_similarity(X=[base_split_emb], Y=[split_emb])\n",
    "        if (\n",
    "            distance[0][0] < merge_threshold\n",
    "            or len(base_split_text) + len(split.page_content) > max_chunk_size\n",
    "        ):\n",
    "            splits.append(Document(page_content=base_split_text))\n",
    "            base_split_text = split.page_content\n",
    "            base_split_emb = split_emb\n",
    "        else:\n",
    "            base_split_text += split.page_content\n",
    "\n",
    "    if base_split_text:\n",
    "        splits.append(Document(page_content=base_split_text))\n",
    "\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HuggingFaceEmbeddings\n",
    "Since it's just an approximation, it's acceptable to use very light embedding models like KLUE, https://huggingface.co/klue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name klue/roberta-small. Creating a new one with mean pooling.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-small and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.54 s, sys: 4.97 s, total: 10.5 s\n",
      "Wall time: 52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "hfembeddings = HuggingFaceEmbeddings(model_name=\"klue/roberta-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SemanticChunker Splits: 248\n",
      "CPU times: user 19.1 s, sys: 3.05 s, total: 22.2 s\n",
      "Wall time: 31.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "semantic_splits = semantic_chunker(docs, merge_threshold=0.8, embeddings=hfembeddings)\n",
    "print(\"SemanticChunker Splits:\", len(semantic_splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAG5CAYAAABoRvUVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABKBElEQVR4nO3deVhUZf8/8PewDPsMogKSsrgi7qLhuCcoKo9mWmm5r4+G9qi50eOGe+auoGmKy1ezrLRyV0KsRHNDzT3DIBUwFRBUULh/f/TjPI6AzsAMMxzer+uaS+c+95zzOTNnZt7ccxaFEEKAiIiISKYsTF0AERERkTEx7BAREZGsMewQERGRrDHsEBERkawx7BAREZGsMewQERGRrDHsEBERkawx7BAREZGsMewQERGRrDHsULly5MgRKBQKHDlypNSX7e3tjX/961+lvlwA2LhxIxQKBU6dOmWS5RtD+/bt0b59e+n+zZs3oVAosHHjRpPVRGQoM2fOhEKhwN9//23qUmSBYaccunDhAt5++214eXnB1tYWr732Gjp27IiVK1eaujSDiYyMLLUvvZSUFEyYMAG+vr6wt7eHg4MD/P39MWfOHKSlpZVKDeYuLy8PmzdvRkBAAFxcXODk5ITatWtjwIABOH78uNGWu3fvXsycOVPn/u3bt4dCoZBuLi4uaN68OTZs2IC8vDyj1WkIptoOb9++jZkzZyI+Pt5oy3jevHnzsGvXLp365gfgRYsWGbeoEtBnfaj4rExdAJWuY8eO4Y033oCnpyeGDx8Od3d3JCUl4fjx41i+fDnGjBlj6hINIjIyEpUqVcKgQYO02tu2bYvHjx9DqVQaZDknT55E165dkZmZiX79+sHf3x8AcOrUKSxYsABHjx7FwYMHDbKssuzDDz9EREQE3nzzTfTt2xdWVla4evUq9u3bh+rVq6NFixYlXoaXlxceP34Ma2trqW3v3r2IiIjQK/BUrVoV8+fPBwDcvXsXmzdvxtChQ3Ht2jUsWLCgxHUagym3w9u3byM8PBze3t5o3LixUZbxvHnz5uHtt99Gjx49jL6s0iC39TFXDDvlzNy5c6FWq3Hy5Ek4OztrTUtNTTVNUaXIwsICtra2BplXWloa3nrrLVhaWuLs2bPw9fXVmj537lysW7fOIMsyd3l5ecjJySn0uU1JSUFkZCSGDx+OtWvXak1btmwZ7t69a5AaFAqFQV5btVqNfv36Sff//e9/o06dOli1ahVmz56tFabMAbdDolfjz1jlzI0bN1CvXr0CQQcAXF1dC7T93//9H/z9/WFnZwcXFxf06dMHSUlJWn3at2+P+vXr4/z582jXrh3s7e1Rs2ZNfP311wCA2NhYBAQEwM7ODnXq1MHhw4e1Hv/nn3/igw8+QJ06dWBnZ4eKFSvinXfewc2bN7X65e938ssvv2D8+PGoXLkyHBwc8NZbb2l9YXp7e+PixYuIjY2Vfo7I37ejqH12Tpw4ga5du6JChQpwcHBAw4YNsXz58pc+l5999hlu3bqFJUuWFPiCAQA3NzdMnTq1QPvPP/+M119/Hba2tqhevTo2b96sNT3/t/oX5a//889L/n5Ar5pnYR48eIDXX38dVatWxdWrVwEA2dnZmDFjBmrWrAkbGxtUq1YNkyZNQnZ2ttZjFQoFRo8eja1bt6JevXqwsbHB/v37C11OQkIChBBo1apVgWkKhUJru8tfx6NHj+Lf//43KlasCJVKhQEDBuDBgwcvXZ8X99kZNGgQIiIipOXk3/Rlb2+PFi1aICsrC3fv3tV5ewUgvSfs7OxQtWpVzJkzB1FRUQVeRwDYt28f2rRpAwcHBzg5OSEkJAQXL158ZX3F2Q4jIyOl183DwwOhoaEFfurKf19funQJb7zxBuzt7fHaa69h4cKFUp8jR46gefPmAIDBgwdLz/HzPyGfOHECnTt3hlqthr29Pdq1a4dffvlFa1n52/zvv/+OQYMGwdnZGWq1GoMHD8ajR4+kfgqFAllZWdi0aZO0rBdHb4tD3+1+165dqF+/PmxsbFCvXr1Ct/0jR46gWbNmsLW1RY0aNfDZZ58VeG/rsj5paWkvfU4A4NChQ2jdujWcnZ3h6OiIOnXq4OOPPy7x8yIrgsqVTp06CScnJ3HhwoVX9p0zZ45QKBSid+/eIjIyUoSHh4tKlSoJb29v8eDBA6lfu3bthIeHh6hWrZqYOHGiWLlypfDz8xOWlpZi+/btwt3dXcycOVMsW7ZMvPbaa0KtVouMjAzp8Tt27BCNGjUS06dPF2vXrhUff/yxqFChgvDy8hJZWVlSv6ioKAFANGnSRHTo0EGsXLlSfPTRR8LS0lK8++67Ur+dO3eKqlWrCl9fX7FlyxaxZcsWcfDgQSGEEDExMQKAiImJkfofPHhQKJVK4eXlJWbMmCFWr14tPvzwQxEUFPTS56dly5bCzs5OZGdnv/K5FEIILy8vUadOHeHm5iY+/vhjsWrVKtG0aVOhUCjEb7/9JvWbMWOGKOytmb/+CQkJes8z/7EnT54UQghx9+5d0bhxY+Hp6Sl+//13IYQQubm5olOnTsLe3l6MHTtWfPbZZ2L06NHCyspKvPnmm1q1ABB169YVlStXFuHh4SIiIkKcPXu20PW+ffu2ACBCQkK0Xs/C5NfZoEED0aZNG7FixQoRGhoqLCwsRNu2bUVeXp7Ut127dqJdu3bS/YSEBAFAREVFCSGEOHbsmOjYsaMAIG0HW7Zseeny27VrJ+rVq1egvWnTpsLS0lJkZWXpvL3+9ddfwsXFRVSsWFGEh4eLRYsWCV9fX9GoUaMCr+PmzZuFQqEQnTt3FitXrhSffPKJ8Pb2Fs7Ozlr9CqPvdpi/fQUFBYmVK1eK0aNHC0tLS9G8eXORk5Oj9Vzkv6//85//iMjISNGhQwcBQOzdu1cIIURycrKYNWuWACBGjBghPcc3btwQQggRHR0tlEql0Gg0YvHixWLp0qWiYcOGQqlUihMnThSoqUmTJqJnz54iMjJSDBs2TAAQkyZNkvpt2bJF2NjYiDZt2kjLOnbsWJHrmr9NfPrpp0X20Xe7b9SokahSpYqYPXu2WLZsmahevbqwt7cXf//9t9TvzJkzwsbGRnh7e4sFCxaIuXPnCg8PD+m112V9dH1OfvvtN6FUKkWzZs3E8uXLxZo1a8SECRNE27Zti1zn8ohhp5w5ePCgsLS0FJaWlkKj0YhJkyaJAwcOaH3ICSHEzZs3haWlpZg7d65W+4ULF4SVlZVWe7t27QQAsW3bNqntypUrAoCwsLAQx48fl9oPHDig9YUkhBCPHj0qUGdcXJwAIDZv3iy15X8RBgUFaX3pjRs3TlhaWoq0tDSprV69elpfhPleDDvPnj0TPj4+wsvLSyvACSG0llGYChUqiEaNGr20z/O8vLwEAHH06FGpLTU1VdjY2IiPPvpIatM37Ogyz+fDzp07d0S9evVE9erVxc2bN6U+W7ZsERYWFuKnn37SWu6aNWsEAPHLL79Ibfmv7cWLF3Va9wEDBggAokKFCuKtt94SixYtEpcvXy5yHf39/bW2yYULFwoA4rvvvpPaXhV2hBAiNDS00OeyKO3atRO+vr7i7t274u7du+Ly5cviww8/FABEt27dhBC6b69jxowRCoVCKwTeu3dPuLi4aL2ODx8+FM7OzmL48OFa80xOThZqtbpA+4v02Q5TU1OFUqkUnTp1Erm5uVL7qlWrBACxYcMGrefixXXKzs4W7u7uolevXlLbyZMnCzzvQvzz/qlVq5YIDg7Wei89evRI+Pj4iI4dO0pt+dv8kCFDtObx1ltviYoVK2q1OTg4iIEDB+q0vrqEHX23e6VSKf2BIIQQ586dEwDEypUrpbZu3boJe3t7cevWLant+vXrwsrKqsD2WNT66PqcLF26VAAQd+/eLXIdSQj+jFXOdOzYEXFxcejevTvOnTuHhQsXIjg4GK+99hq+//57qd+3336LvLw8vPvuu/j777+lm7u7O2rVqoWYmBit+To6OqJPnz7S/Tp16sDZ2Rl169ZFQECA1J7//z/++ENqs7Ozk/7/9OlT3Lt3DzVr1oSzszPOnDlTYB1GjBihNRTcpk0b5Obm4s8//9T7+Th79iwSEhIwduzYAj/tveonj4yMDDg5Oem1PD8/P7Rp00a6X7lyZdSpU0fr+dCXPvP866+/0K5dOzx9+hRHjx6Fl5eXNG3Hjh2oW7cufH19tV7zDh06AECB17xdu3bw8/PTqcaoqCisWrUKPj4+2LlzJyZMmIC6desiMDAQt27dKtB/xIgRWvvGjBo1ClZWVti7d69OyyuJK1euoHLlyqhcuTLq1q2LlStXIiQkBBs2bACg+/a6f/9+aDQarZ12XVxc0LdvX63lHTp0CGlpaXjvvfe0nndLS0sEBAQUeN5fpM92ePjwYeTk5GDs2LGwsPjfx//w4cOhUqmwZ88erf6Ojo5a+y8plUq8/vrrOm2v8fHxuH79Ot5//33cu3dPWq+srCwEBgbi6NGjBY5wGzlypNb9Nm3a4N69e8jIyNBp/YpD3+0+KCgINWrUkO43bNgQKpVKek5yc3Nx+PBh9OjRAx4eHlK/mjVrokuXLnrX96rnJP9z67vvvjP7IwZNiTsol0PNmzfHt99+i5ycHJw7dw47d+7E0qVL8fbbbyM+Ph5+fn64fv06hBCoVatWofN4cSfNqlWrFggHarUa1apVK9AGQGv/i8ePH2P+/PmIiorCrVu3IISQpqWnpxdYtqenp9b9ChUqFJinrm7cuAEAqF+/vt6PValUePjwoV6PebF24J/6i1N7cebZv39/WFlZ4fLly3B3d9eadv36dVy+fBmVK1cudDkv7sDu4+Ojc40WFhYIDQ1FaGgo7t27h19++QVr1qzBvn370KdPH/z0009a/V/c7hwdHVGlSpVC94sxNG9vb6xbt07a4blWrVpa+xXpur3++eef0Gg0BeZfs2ZNrfvXr18HAOnL9UUqleql9eqzHeb/QVCnTh2tdqVSierVqxf4g6Gw93WFChVw/vz5Vy4rf70GDhxYZJ/09HTp/Qu8/L39quehuPTd7l/1fktNTcXjx48LvM5AwddeF696Tnr37o3PP/8cw4YNw5QpUxAYGIiePXvi7bff1gq05R3DTjmmVCrRvHlzNG/eHLVr18bgwYOxY8cOzJgxA3l5eVAoFNi3bx8sLS0LPNbR0VHrfmF9Xtb+/BfEmDFjEBUVhbFjx0Kj0UCtVkOhUKBPnz6F/qWiyzxLg6+vL+Lj45GTk6Pzoey61F7UiFJubm6x55mvZ8+e2Lx5M5YvXy4dXp0vLy8PDRo0wJIlSwqd34vB9fkRDn1UrFgR3bt3R/fu3dG+fXvExsbizz//1BplMiUHBwcEBQUVOV3f7fVV8h+zZcuWAgEUAKysXv4xXZztUFclea/lr9enn35a5CHpun6OGPO9re92X9o1vmp5dnZ2OHr0KGJiYrBnzx7s378fX375JTp06ICDBw8W+fjyhmGHAADNmjUDANy5cwcAUKNGDQgh4OPjg9q1axt12V9//TUGDhyIxYsXS21Pnjwp0YnQdD3qJn84+rfffnvpF1xhunXrhri4OHzzzTd477339K6xKPl/uaWlpWn9tFacn+leNGbMGNSsWRPTp0+HWq3GlClTpGk1atTAuXPnEBgYWKyjloqjWbNmiI2NxZ07d7TCzvXr1/HGG29I9zMzM3Hnzh107dpVr/kbYz103V69vLzw+++/F3j8i23526Crq6ve2yCg33aY/xxfvXoV1atXl9pzcnKQkJBQrOUX9Rznr5dKpSrWfPVdXnEZert3dXWFra2tTq89YJj1sbCwQGBgIAIDA7FkyRLMmzcP//3vfxETE2PQ574s4xhXORMTE1PoXyD5+0LkD2/37NkTlpaWCA8PL9BfCIF79+4ZrCZLS8sCy1i5cmWRIxm6cHBw0CksNW3aFD4+Pli2bFmB/q/6S23kyJGoUqUKPvroI1y7dq3A9NTUVMyZM0efsgH870vi6NGjUlv+4amGMG3aNEyYMAFhYWFYvXq11P7uu+/i1q1bhZ6T5fHjx8jKyirW8pKTk3Hp0qUC7Tk5OYiOjoaFhUWB4f21a9fi6dOn0v3Vq1fj2bNneu/z4ODgAAAGPYOwrttrcHAw4uLitM4sfP/+fWzdurVAP5VKhXnz5mmtc75XnYdIn+0wKCgISqUSK1as0FqH9evXIz09HSEhIS9dVmGKeo79/f1Ro0YNLFq0CJmZmQUeV9zzK+n63taVobd7S0tLBAUFYdeuXbh9+7bU/vvvv2Pfvn0F+pd0fe7fv1+gLX8k7cVD58szjuyUM2PGjMGjR4/w1ltvwdfXFzk5OTh27Bi+/PJLeHt7Y/DgwQD++cKdM2cOwsLCcPPmTfTo0QNOTk5ISEjAzp07MWLECEyYMMEgNf3rX//Cli1boFar4efnh7i4OBw+fBgVK1Ys9jz9/f2xevVqzJkzBzVr1oSrq2uh+0RYWFhg9erV6NatGxo3bozBgwejSpUquHLlCi5evIgDBw4UuYwKFSpg586d6Nq1Kxo3bqx15tozZ87giy++KHSfjVfp1KkTPD09MXToUEycOBGWlpbYsGEDKleujMTERL3nV5hPP/0U6enpCA0NhZOTE/r164f+/fvjq6++wsiRIxETE4NWrVohNzcXV65cwVdffYUDBw5II4D6+Ouvv/D666+jQ4cOCAwMhLu7O1JTU/HFF1/g3LlzGDt2LCpVqqT1mJycHAQGBuLdd9/F1atXERkZidatW6N79+56LTv/9fjwww8RHBwMS0tLrR3pi0PX7XXSpEn4v//7P3Ts2BFjxoyBg4MDPv/8c3h6euL+/fvSX/QqlQqrV69G//790bRpU/Tp00d6rffs2YNWrVph1apVRdajz3ZYuXJlhIWFITw8HJ07d0b37t2l57d58+ZaOyPrqkaNGnB2dsaaNWvg5OQEBwcHBAQEwMfHB59//jm6dOmCevXqYfDgwXjttddw69YtxMTEQKVS4YcfftB7ef7+/jh8+DCWLFkCDw8P+Pj4aB0EUZjo6Gg8efKkQHuPHj2Mst3PnDkTBw8eRKtWrTBq1Cjk5uZi1apVqF+/foHLahRnfZ43a9YsHD16FCEhIfDy8kJqaioiIyNRtWpVtG7dWq+6Za2Uj/4iE9u3b58YMmSI8PX1FY6OjkKpVIqaNWuKMWPGiJSUlAL9v/nmG9G6dWvh4OAgHBwchK+vrwgNDRVXr16V+hR1bhIvLy8REhJSoB2ACA0Nle4/ePBADB48WFSqVEk4OjqK4OBgceXKFeHl5aV1SOaL54rJV9i5c5KTk0VISIhwcnISAKRDlAvrK4QQP//8s+jYsaNwcnISDg4OomHDhlqHkr7M7du3xbhx40Tt2rWFra2tsLe3F/7+/mLu3LkiPT39lc/Hi4dQCyHE6dOnRUBAgFAqlcLT01MsWbKkyEPPdZlnYc9dbm6ueO+994SVlZXYtWuXEEKInJwc8cknn4h69eoJGxsbUaFCBeHv7y/Cw8O11uXF1/BlMjIyxPLly0VwcLCoWrWqsLa2Fk5OTkKj0Yh169ZpHZacX2dsbKwYMWKEqFChgnB0dBR9+/YV9+7de+k6Fnbo+bNnz8SYMWNE5cqVhUKheOVh6EVty8/TdXsVQoizZ8+KNm3aCBsbG1G1alUxf/58sWLFCgFAJCcna/WNiYkRwcHBQq1WC1tbW1GjRg0xaNAgcerUqZfWk0/X7VCIfw419/X1FdbW1sLNzU2MGjWqwKkXinouBg4cKLy8vLTavvvuO+Hn5ycdWv38a3D27FnRs2dPUbFiRWFjYyO8vLzEu+++K6Kjo6U++YdZv3j4dGHb/JUrV0Tbtm2FnZ2dAPDSw9Dzt4mibvnnXSrpdl/Yax8dHS2aNGkilEqlqFGjhvj888/FRx99JGxtbbX6FbU+uj4n0dHR4s033xQeHh5CqVQKDw8P8d5774lr164V+byURwohSnmvTiKiImzcuBGDBw/GyZMnizWKVBaMHTsWn332GTIzM7nzaDnTo0cPXLx4UTpSjUoP99khIjKSx48fa92/d+8etmzZgtatWzPoyNyLr/3169exd+9e6dI1VLq4zw4RkZFoNBq0b98edevWRUpKCtavX4+MjAxMmzbN1KWRkVWvXh2DBg2Szl+0evVqKJVKTJo0ydSllUsMO0RERtK1a1d8/fXXWLt2LRQKBZo2bYr169ejbdu2pi6NjKxz58744osvkJycDBsbG2g0GsybN6/IE7WScXGfHSIiIpI17rNDREREssawQ0RERLLGfXbwz7VRbt++DScnp1I7TT4RERGVjBACDx8+hIeHx0svfMqwA+D27dsFLvZGREREZUNSUhKqVq1a5HSGHQBOTk4A/nmyVCqViashIiIiXWRkZKBatWrS93hRGHYArWvUMOwQERGVLa/aBYU7KBMREZGsMewQERGRrDHsEBERkawx7BAREZGsMewQERGRrDHsEBERkawx7BAREZGsMewQERGRrDHsEBERkawx7BAREZGsMewQERGRrDHsEBERkawx7BAREZGsMewQERGRrDHsEBERkawx7BBRmeY9ZY+pSyAiM8ewQ0RERLLGsENERESyxrBDREREssawQ0RERLLGsENERESyxrBDREREssawQ0RERLLGsENERESyxrBDREREssawQ0RERLLGsENERESyxrBDREREssawQ/QKvNAkEVHZxrBDREREssawQ0RERLLGsENERESyxrBDREREssawQ0RERLLGsENERESyxrBDREREsmZl6gKIzBXPr0NEJA8c2SEiIiJZY9ghIiIiWWPYISIiIllj2CEiIiJZM3nYuXXrFvr164eKFSvCzs4ODRo0wKlTp6TpQghMnz4dVapUgZ2dHYKCgnD9+nWtedy/fx99+/aFSqWCs7Mzhg4diszMzNJeFSIiIjJDJg07Dx48QKtWrWBtbY19+/bh0qVLWLx4MSpUqCD1WbhwIVasWIE1a9bgxIkTcHBwQHBwMJ48eSL16du3Ly5evIhDhw5h9+7dOHr0KEaMGGGKVSIiIiIzY9JDzz/55BNUq1YNUVFRUpuPj4/0fyEEli1bhqlTp+LNN98EAGzevBlubm7YtWsX+vTpg8uXL2P//v04efIkmjVrBgBYuXIlunbtikWLFsHDw6N0V4qIiIjMiklHdr7//ns0a9YM77zzDlxdXdGkSROsW7dOmp6QkIDk5GQEBQVJbWq1GgEBAYiLiwMAxMXFwdnZWQo6ABAUFAQLCwucOHGi9FaGiIiIzJJJw84ff/yB1atXo1atWjhw4ABGjRqFDz/8EJs2bQIAJCcnAwDc3Ny0Hufm5iZNS05Ohqurq9Z0KysruLi4SH1elJ2djYyMDK0bERERyZNJf8bKy8tDs2bNMG/ePABAkyZN8Ntvv2HNmjUYOHCg0ZY7f/58hIeHG23+REREZD5MOrJTpUoV+Pn5abXVrVsXiYmJAAB3d3cAQEpKilaflJQUaZq7uztSU1O1pj979gz379+X+rwoLCwM6enp0i0pKckg60NERETmx6Rhp1WrVrh69apW27Vr1+Dl5QXgn52V3d3dER0dLU3PyMjAiRMnoNFoAAAajQZpaWk4ffq01OfHH39EXl4eAgICCl2ujY0NVCqV1o2IiIjkyaQ/Y40bNw4tW7bEvHnz8O677+LXX3/F2rVrsXbtWgCAQqHA2LFjMWfOHNSqVQs+Pj6YNm0aPDw80KNHDwD/jAR17twZw4cPx5o1a/D06VOMHj0affr04ZFYREREZNqw07x5c+zcuRNhYWGYNWsWfHx8sGzZMvTt21fqM2nSJGRlZWHEiBFIS0tD69atsX//ftja2kp9tm7ditGjRyMwMBAWFhbo1asXVqxYYYpVIiIiIjOjEEIIUxdhahkZGVCr1UhPT+dPWiTxnrJH+v/NBSEmrIRexnvKHr4+ROWUrt/fJr9cBBEREZExMewQERGRrDHsEBERkawx7BAREZGsMewQERGRrDHsEBERkawx7BAREZGsMewQERGRrDHsEBERkawx7BAREZGsMewQERGRrDHsEBERkawx7BAREZGsMewQERGRrDHsEBERkawx7BAREZGsMewQERGRrDHsEBERkawx7BAREZGsMewQERGRrDHsEBERkawx7BAREZGsMewQERGRrDHsEBERkawx7BDpyHvKHlOXQERExcCwQ0RERLLGsENERESyxrBDREREssawQ0RERLLGsENERESyxrBDREREssawQ0RERLLGsENERESyxrBDREREssawQ0RERLLGsENERESyxrBDREREssawQ0RERLLGsENERESyxrBDREREssawQ0RERLLGsENERESyxrBDREREsmbSsDNz5kwoFAqtm6+vrzT9yZMnCA0NRcWKFeHo6IhevXohJSVFax6JiYkICQmBvb09XF1dMXHiRDx79qy0V4WIiIjMlJWpC6hXrx4OHz4s3bey+l9J48aNw549e7Bjxw6o1WqMHj0aPXv2xC+//AIAyM3NRUhICNzd3XHs2DHcuXMHAwYMgLW1NebNm1fq60JERETmx+Rhx8rKCu7u7gXa09PTsX79emzbtg0dOnQAAERFRaFu3bo4fvw4WrRogYMHD+LSpUs4fPgw3Nzc0LhxY8yePRuTJ0/GzJkzoVQqS3t1iIiIyMyYfJ+d69evw8PDA9WrV0ffvn2RmJgIADh9+jSePn2KoKAgqa+vry88PT0RFxcHAIiLi0ODBg3g5uYm9QkODkZGRgYuXrxY5DKzs7ORkZGhdSMiIiJ5MmnYCQgIwMaNG7F//36sXr0aCQkJaNOmDR4+fIjk5GQolUo4OztrPcbNzQ3JyckAgOTkZK2gkz89f1pR5s+fD7VaLd2qVatm2BUjIiIis2HSn7G6dOki/b9hw4YICAiAl5cXvvrqK9jZ2RltuWFhYRg/frx0PyMjg4GHiIhIpkz+M9bznJ2dUbt2bfz+++9wd3dHTk4O0tLStPqkpKRI+/i4u7sXODor/35h+wHls7GxgUql0roRERGRPJlV2MnMzMSNGzdQpUoV+Pv7w9raGtHR0dL0q1evIjExERqNBgCg0Whw4cIFpKamSn0OHToElUoFPz+/Uq+fiIiIzI9Jf8aaMGECunXrBi8vL9y+fRszZsyApaUl3nvvPajVagwdOhTjx4+Hi4sLVCoVxowZA41GgxYtWgAAOnXqBD8/P/Tv3x8LFy5EcnIypk6ditDQUNjY2Jhy1YiIiMhMmDTs/PXXX3jvvfdw7949VK5cGa1bt8bx48dRuXJlAMDSpUthYWGBXr16ITs7G8HBwYiMjJQeb2lpid27d2PUqFHQaDRwcHDAwIEDMWvWLFOtEhEREZkZk4ad7du3v3S6ra0tIiIiEBERUWQfLy8v7N2719ClERERkUyY1T47RERERIbGsENERESyxrBDREREssawQ0RERLLGsENERESyxrBDREREssawQ0RERLJWrPPsREdHIzo6GqmpqcjLy9OatmHDBoMURkRERGQIeoed8PBwzJo1C82aNUOVKlWgUCiMURcRERGRQegddtasWYONGzeif//+xqiHiIiIyKD03mcnJycHLVu2NEYtRERERAand9gZNmwYtm3bZoxaiIiIiAxOp5+xxo8fL/0/Ly8Pa9euxeHDh9GwYUNYW1tr9V2yZIlhKyQiIiIqAZ3CztmzZ7XuN27cGADw22+/GbwgIiIiIkPSKezExMQYuw4iIiIio9B7n50hQ4bg4cOHBdqzsrIwZMgQgxRFREREZCh6h51Nmzbh8ePHBdofP36MzZs3G6QoIiIiIkPR+Tw7GRkZEEJACIGHDx/C1tZWmpabm4u9e/fC1dXVKEUSERERFZfOYcfZ2RkKhQIKhQK1a9cuMF2hUCA8PNygxRERERGVlM5hJyYmBkIIdOjQAd988w1cXFykaUqlEl5eXvDw8DBKkURERETFpXPYadeuHQAgISEBnp6evCYWERERlQl6XxsrPT0dFy5cKNCuUChga2sLT09P2NjYGKQ4IiIiopLSO+w0btz4paM61tbW6N27Nz777DOtnZiJiIiITEHvQ8937tyJWrVqYe3atYiPj0d8fDzWrl2LOnXqYNu2bVi/fj1+/PFHTJ061Rj1EhEREelF75GduXPnYvny5QgODpbaGjRogKpVq2LatGn49ddf4eDggI8++giLFi0yaLFERESkO+8pe3BzQYipyzA5vUd2Lly4AC8vrwLtXl5e0r48jRs3xp07d0peHREREVEJ6R12fH19sWDBAuTk5EhtT58+xYIFC+Dr6wsAuHXrFtzc3AxXJREREVEx6f0zVkREBLp3746qVauiYcOGAP4Z7cnNzcXu3bsBAH/88Qc++OADw1ZKREREVAx6h52WLVsiISEBW7duxbVr1wAA77zzDt5//304OTkBAPr372/YKomISHa4PwmVFr3DDgA4OTlh5MiRhq6FiIiIyOCKFXauX7+OmJgYpKamIi8vT2va9OnTDVIYERFRcXlP2QMAHDkiAMUIO+vWrcOoUaNQqVIluLu7a51gUKFQMOwQERGRWdE77MyZMwdz587F5MmTjVEPERERkUHpfej5gwcP8M477xijFiIiIiKD0zvsvPPOOzh48KAxaiEiIiIyOL1/xqpZsyamTZuG48ePo0GDBrC2ttaa/uGHHxqsOCIiIqKS0jvsrF27Fo6OjoiNjUVsbKzWNIVCwbBDREREZkXvsJOQkGCMOoiIiIiMQu99dvLl5OTg6tWrePbsmSHrISIiIjIovcPOo0ePMHToUNjb26NevXpITEwEAIwZMwYLFiwweIFEREREJaF32AkLC8O5c+dw5MgR2NraSu1BQUH48ssvDVocERERUUnpvc/Orl278OWXX6JFixZaZ0+uV68ebty4YdDiiIiIyPTK+uU39B7ZuXv3LlxdXQu0Z2VlaYUffS1YsAAKhQJjx46V2p48eYLQ0FBUrFgRjo6O6NWrF1JSUrQel5iYiJCQENjb28PV1RUTJ07kfkRERFQueU/ZIwUT+h+9w06zZs2wZ8//nsj8gPP5559Do9EUq4iTJ0/is88+Q8OGDbXax40bhx9++AE7duxAbGwsbt++jZ49e0rTc3NzERISgpycHBw7dgybNm3Cxo0beX0uIiIikuj9M9a8efPQpUsXXLp0Cc+ePcPy5ctx6dIlHDt2rMB5d3SRmZmJvn37Yt26dZgzZ47Unp6ejvXr12Pbtm3o0KEDACAqKgp169bF8ePH0aJFCxw8eBCXLl3C4cOH4ebmhsaNG2P27NmYPHkyZs6cCaVSqXc9REREZR1Hd7TpPbLTunVrxMfH49mzZ2jQoAEOHjwIV1dXxMXFwd/fX+8CQkNDERISgqCgIK3206dP4+nTp1rtvr6+8PT0RFxcHAAgLi4ODRo0gJubm9QnODgYGRkZuHjxot61EBkDh5WJiExL75EdAKhRowbWrVun1Zaamop58+bh448/1nk+27dvx5kzZ3Dy5MkC05KTk6FUKuHs7KzV7ubmhuTkZKnP80Enf3r+tKJkZ2cjOztbup+RkaFzzURERFS2FPukgi+6c+cOpk2bpnP/pKQk/Oc//8HWrVu1DmEvDfPnz4darZZu1apVK9XlExERUekxWNjR1+nTp5GamoqmTZvCysoKVlZWiI2NxYoVK2BlZQU3Nzfk5OQgLS1N63EpKSlwd3cHALi7uxc4Oiv/fn6fwoSFhSE9PV26JSUlGXbliPTEn7qIiIzHZGEnMDAQFy5cQHx8vHRr1qwZ+vbtK/3f2toa0dHR0mOuXr2KxMRE6agvjUaDCxcuIDU1Vepz6NAhqFQq+Pn5FblsGxsbqFQqrRsRERHJU7H22TEEJycn1K9fX6vNwcEBFStWlNqHDh2K8ePHw8XFBSqVCmPGjIFGo0GLFi0AAJ06dYKfnx/69++PhQsXIjk5GVOnTkVoaChsbGxKfZ2IiIjI/OgcdsaPH//S6Xfv3i1xMS9aunQpLCws0KtXL2RnZyM4OBiRkZHSdEtLS+zevRujRo2CRqOBg4MDBg4ciFmzZhm8FiIiIiqbdA47Z8+efWWftm3blqiYI0eOaN23tbVFREQEIiIiinyMl5cX9u7dW6LlEhERkXzpHHZiYmKMWQcRERGRUZhsB2UiIiKi0sCwQ0RERLLGsENERESyxrBDRERUDpWnk5nqfZ6dxMREVKtWDQqFQqtdCIGkpCR4enoarDgiIiIyHbmEIb1Hdnx8fAo9p879+/fh4+NjkKKIiIiIDEXvsCOEKDCqAwCZmZmlfkFPIiIiolfR+wzKCoUC06ZNg729vTQtNzcXJ06cQOPGjQ1eIBEREVFJ6H0GZSEELly4AKVSKU1TKpVo1KgRJkyYYPgKiYiIiEpA7zMoDx48GMuXL+eVwomIiKhM0PtorKioKGPUQURERGQUeoedrKwsLFiwANHR0UhNTUVeXp7W9D/++MNgxREREVHJeU/Zg5sLQkxdhsnoHXaGDRuG2NhY9O/fH1WqVCn0yCwiIiIic6F32Nm3bx/27NmDVq1aGaMeIiIiIoPS+zw7FSpUgIuLizFqISIzV55OL09E8qF32Jk9ezamT5+OR48eGaMeIiIiIoPS+2esxYsX48aNG3Bzc4O3tzesra21pp85c8ZgxRERERGVlN5hp0ePHkYog4iIiMg49A47M2bMMEYdREREREah9z47AJCWlobPP/8cYWFhuH//PoB/fr66deuWQYsjIiIiKim9R3bOnz+PoKAgqNVq3Lx5E8OHD4eLiwu+/fZbJCYmYvPmzcaok4iIiKhY9B7ZGT9+PAYNGoTr16/D1tZWau/atSuOHj1q0OKIiIiISkrvsHPy5En8+9//LtD+2muvITk52SBFERERERmK3mHHxsYGGRkZBdqvXbuGypUrG6QoIiIiIkPRO+x0794ds2bNwtOnTwEACoUCiYmJmDx5Mnr16mXwAomIiIhKQu+ws3jxYmRmZsLV1RWPHz9Gu3btULNmTTg5OWHu3LnGqJGIiIio2PQ+GkutVuPQoUP45ZdfcO7cOWRmZqJp06YICgoyRn1EREREJaL3yM7mzZuRnZ2NVq1a4YMPPsCkSZMQFBSEnJwcHnZOREQkY2X1QsB6h53BgwcjPT29QPvDhw8xePBggxRFREREZCh6hx0hBBQKRYH2v/76C2q12iBFERERERmKzvvsNGnSBAqFAgqFAoGBgbCy+t9Dc3NzkZCQgM6dOxulSCIiIqLi0jns5F/tPD4+HsHBwXB0dJSmKZVKeHt789BzIiIiMjs6h538q517e3ujd+/eWpeKICpPvKfswc0FIaYug4iIdKT3oecDBw4EAOTk5CA1NRV5eXla0z09PQ1TGRGRHhhCiXRTVo+oKgm9w87169cxZMgQHDt2TKs9f8fl3NxcgxVHREREVFJ6h51BgwbBysoKu3fvRpUqVQo9MouIiIjIXOgdduLj43H69Gn4+voaox4iIiIig9L7PDt+fn74+++/jVELERERkcHpHXY++eQTTJo0CUeOHMG9e/eQkZGhdSMiIiIyJ3r/jJV/wc/AwECtdu6gTEREROZI77ATExNjjDqIiIiIjELvsNOuXTtj1EFERERkFHrvswMAaWlpWLx4MYYNG4Zhw4Zh6dKlhV4J/VVWr16Nhg0bQqVSQaVSQaPRYN++fdL0J0+eIDQ0FBUrVoSjoyN69eqFlJQUrXkkJiYiJCQE9vb2cHV1xcSJE/Hs2bPirBYRERHJkN5h59SpU6hRowaWLl2K+/fv4/79+1iyZAlq1KiBM2fO6DWvqlWrYsGCBTh9+jROnTqFDh064M0338TFixcBAOPGjcMPP/yAHTt2IDY2Frdv30bPnj2lx+fm5iIkJAQ5OTk4duwYNm3ahI0bN2L69On6rhYRERHJlN4/Y40bNw7du3fHunXrpCufP3v2DMOGDcPYsWNx9OhRnefVrVs3rftz587F6tWrcfz4cVStWhXr16/Htm3b0KFDBwBAVFQU6tati+PHj6NFixY4ePAgLl26hMOHD8PNzQ2NGzfG7NmzMXnyZMycORNKpVLf1SMiIiKZKdbIzuTJk6WgAwBWVlaYNGkSTp06VexCcnNzsX37dmRlZUGj0eD06dN4+vSpdPQXAPj6+sLT0xNxcXEAgLi4ODRo0ABubm5Sn+DgYGRkZEijQ4XJzs7mIfNERETlhN5hR6VSITExsUB7UlISnJyc9C7gwoULcHR0hI2NDUaOHImdO3fCz88PycnJUCqVcHZ21urv5uaG5ORkAEBycrJW0Mmfnj+tKPPnz4darZZu1apV07tuIqKXKY8XWyQyV3qHnd69e2Po0KH48ssvkZSUhKSkJGzfvh3Dhg3De++9p3cBderUQXx8PE6cOIFRo0Zh4MCBuHTpkt7z0UdYWBjS09OlW1JSklGXR0RE9DzvKXsYiEuR3vvsLFq0CAqFAgMGDJCOerK2tsaoUaOwYMECvQtQKpWoWbMmAMDf3x8nT57E8uXL0bt3b+Tk5CAtLU1rdCclJQXu7u4AAHd3d/z6669a88s/Wiu/T2FsbGxgY2Ojd61EJZH/wXZzQYiJKyEiKl/0HtlRKpVYvnw5Hjx4gPj4eMTHx+P+/ftYunSpQQJEXl4esrOz4e/vD2tra0RHR0vTrl69isTERGg0GgCARqPBhQsXkJqaKvU5dOgQVCoV/Pz8SlwLERERlX06j+zk5ubi4sWLqFWrFuzs7GBvb48GDRoAAB4/fozz58+jfv36sLDQPT+FhYWhS5cu8PT0xMOHD7Ft2zYcOXIEBw4cgFqtxtChQzF+/Hi4uLhApVJhzJgx0Gg0aNGiBQCgU6dO8PPzQ//+/bFw4UIkJydj6tSpCA0N5cgNERERAdBjZGfLli0YMmRIoYdzW1tbY8iQIdi2bZteC09NTcWAAQNQp04dBAYG4uTJkzhw4AA6duwIAFi6dCn+9a9/oVevXmjbti3c3d3x7bffSo+3tLTE7t27YWlpCY1Gg379+mHAgAGYNWuWXnUQERGRfOk8srN+/XpMmDABlpaWBWfy/w89X7VqFfr166fzwtevX//S6ba2toiIiEBERESRfby8vLB3716dl0lkzrhfDxGR4ekcdq5evSr9fFSY5s2b4/LlywYpioiorOIRNkTmR+efsbKysl568r2HDx/i0aNHBimKqKzjFx4RkfnQOezUqlULx44dK3L6zz//jFq1ahmkKCIiIjINOf6xpnPYef/99zF16lScP3++wLRz585h+vTpeP/99w1aHBEREVFJ6bzPzrhx47Bv3z74+/sjKCgIvr6+AIArV67g8OHDaNWqFcaNG2e0QonKE+6oTERkODqHHWtraxw8eBBLly7Ftm3bcPToUQghULt2bcydOxdjx46FtbW1MWslIiIi0ptel4uwtrbGpEmTMGnSJGPVQ0RERGRQel8ugkiO5LhDHhGRLsrD5x/DDhHppDx8IJL88OriBDDsEJlMWfkA1qfOsrJORFS+MOxQqZL7l6Eu6yf354DI1PgeoxfpHXZmzZpV6JmSHz9+zAtwEhERkdnRO+yEh4cjMzOzQPujR48QHh5ukKKI6B/8C5WofOC+Rcald9gRQkChUBRoP3fuHFxcXAxSFBGVXfzQJiJzo/N5dipUqACFQgGFQoHatWtrBZ7c3FxkZmZi5MiRRimSyFi8p+zhWYqJiGRO57CzbNkyCCEwZMgQhIeHQ61WS9OUSiW8vb2h0WiMUiQRERFRcekcdgYOHAgA8PHxQcuWLXlpCNILR1CIqLzhNe7Mh0777GRkZEj/b9KkCR4/foyMjIxCb1S+cX+NsoWvFRGVBzqN7FSoUAF37tyBq6srnJ2dC91BOX/H5dzcXIMXSWUP/6Ip+zgaR0RyoVPY+fHHH6UjrWJiYoxaENHzXhx54JcvERHpS6ew065du0L/T2TOODJBRESAjmHn/PnzOs+wYcOGxS6GdMcvciIyBHP4LDGHGkjedAo7jRs3hkKhgBDipf24zw4RERGZG53CTkJCgrHrIKJi4s7gROUD3+vFp1PY8fLyMnYdJFMcniYifZTmFzrDQ/mh80kFn3f16lWsXLkSly9fBgDUrVsXY8aMQZ06dQxaHBEREVFJ6X0h0G+++Qb169fH6dOn0ahRIzRq1AhnzpxB/fr18c033xijRiIig+BJFMsvvvblm95hZ9KkSQgLC0NcXByWLFmCJUuW4NixY/j4448xadIkY9RI5YQ5fBiZQw1ExPciGZbeYefOnTsYMGBAgfZ+/frhzp07BimKiIiIyFD03menffv2+Omnn1CzZk2t9p9//hlt2rQxWGFERERyxxGs0qF32OnevTsmT56M06dPo0WLFgCA48ePY8eOHQgPD8f333+v1ZeIiIjIlPQOOx988AEAIDIyEpGRkYVOA3iCQSIiIjIPeu+zk5eXp9ONQcf0vKfs4RApEZUKftaQOdM77BAREb0Kww+ZE53DTlxcHHbv3q3VtnnzZvj4+MDV1RUjRoxAdna2wQskIiIi81UWfkXQOezMmjULFy9elO5fuHABQ4cORVBQEKZMmYIffvgB8+fPN0qRVDY2JiIyf/wsofJI57ATHx+PwMBA6f727dsREBCAdevWYfz48VixYgW++uoroxRJREREVFw6H4314MEDuLm5SfdjY2PRpUsX6X7z5s2RlJRk2OqIiIjKOY7ElZzOIztubm5ISEgAAOTk5ODMmTPSeXYA4OHDh7C2tjZ8hUREREQloHPY6dq1K6ZMmYKffvoJYWFhsLe31zpj8vnz51GjRg2jFEnmjX91EBGROdM57MyePRtWVlZo164d1q1bh3Xr1kGpVErTN2zYgE6dOhmlSKKi6Bu0uHOmYfB5JDIuvr8MS+d9dipVqoSjR48iPT0djo6OsLS01Jq+Y8cOODo6GrxAufCesgc3F4SYugxJ/hvJnGoiIiIyBr1PKqhWqwsEHQBwcXHRGunRxfz589G8eXM4OTnB1dUVPXr0wNWrV7X6PHnyBKGhoahYsSIcHR3Rq1cvpKSkaPVJTExESEgI7O3t4erqiokTJ+LZs2f6rprZKG+JvrytLxERlS6TnkE5NjYWoaGhOH78OA4dOoSnT5+iU6dOyMrKkvqMGzcOP/zwA3bs2IHY2Fjcvn0bPXv2lKbn5uYiJCQEOTk5OHbsGDZt2oSNGzdi+vTpplglIiIiMjN6XwjUkPbv3691f+PGjXB1dcXp06fRtm1bpKenY/369di2bRs6dOgAAIiKikLdunVx/PhxtGjRAgcPHsSlS5dw+PBhuLm5oXHjxpg9ezYmT56MmTNn6j3aRERERPJiVtfGSk9PB/DPT2IAcPr0aTx9+hRBQUFSH19fX3h6eiIuLg7AP5exaNCggdY5gIKDg5GRkaF1xmciIiIqn0w6svO8vLw8jB07Fq1atUL9+vUBAMnJyVAqlXB2dtbq6+bmhuTkZKnP80Enf3r+tMJkZ2drXccrIyPDUKtBREREZsZsRnZCQ0Px22+/Yfv27UZf1vz586FWq6VbtWrVjL5MIiIiMg2zCDujR4/G7t27ERMTg6pVq0rt7u7uyMnJQVpamlb/lJQUuLu7S31ePDor/35+nxeFhYUhPT1duvEyF+UPzxNDRFR+mDTsCCEwevRo7Ny5Ez/++CN8fHy0pvv7+8Pa2hrR0dFS29WrV5GYmAiNRgMA0Gg0uHDhAlJTU6U+hw4dgkqlgp+fX6HLtbGxgUql0roRERGRPJl0n53Q0FBs27YN3333HZycnKR9bNRqNezs7KBWqzF06FCMHz8eLi4uUKlUGDNmDDQajXRdrk6dOsHPzw/9+/fHwoULkZycjKlTpyI0NBQ2NjamXD0igvxPYCn39SOSA5OGndWrVwMA2rdvr9UeFRWFQYMGAQCWLl0KCwsL9OrVC9nZ2QgODkZkZKTU19LSErt378aoUaOg0Wjg4OCAgQMHYtasWaW1GkRERGTGTBp2hBCv7GNra4uIiAhEREQU2cfLywt79+41ZGlEpANzuwwKEVFhzGIHZSIiIiJjYdghIiIiWWPYISIiIllj2CGiMonnSSIiXTHsEBGRLJSXAFxe1tOQGHaIiIhI1hh2iMgk+NcpEZUWhh0iIiKSNYYdIiIikjWTnkGZyBzx5xUiInnhyA4RlQqGSCIyFYYdIiIqN+QYuuW4TobGsFPOeU/ZwzcKEemNnx1UljDsEBERkawx7JBs8C9NMnfcRolMg2GHiIiIDMYcAz3DDhEREckaww4RyZY5/oVJRKWPYYeIiIhkjWGHiIiIZI1hh7Rw2J+IiOSGYYeIiIhkjWGHzA5Hl4iIyJAYdohkjCexIyJdyfmzgmGHiIhKlZy/VMk8MexQqeNog+nweSei8ohhx4wwBBARaeNnIhkCww5RMTCYyhdfVyL5fcYx7BAREZGsMewQERGRrDHsEBERkawx7BiZ3H73pLKD213ZxdeOyLAYdswAAxGRYZSH99Lz61ce1pfIEBh2iIiISNYYdoiIShlHZIhKF8MOERERyRrDDlEZwtEAIiL9MeyQ7DAQEGnje4LKOytTF0AkB/wyISIyXww7ZJYYHoiIyFD4MxZROccjg+SHryeRNoYdIiIyewxwVBIMO2UA3+RlF18788ORLDIVbnemY9Kwc/ToUXTr1g0eHh5QKBTYtWuX1nQhBKZPn44qVarAzs4OQUFBuH79ulaf+/fvo2/fvlCpVHB2dsbQoUORmZlZimthWnzzkKEwBLwanx/zYo7brLnVQ/8wadjJyspCo0aNEBERUej0hQsXYsWKFVizZg1OnDgBBwcHBAcH48mTJ1Kfvn374uLFizh06BB2796No0ePYsSIEaW1CkRERGTmTHo0VpcuXdClS5dCpwkhsGzZMkydOhVvvvkmAGDz5s1wc3PDrl270KdPH1y+fBn79+/HyZMn0axZMwDAypUr0bVrVyxatAgeHh6lti5ERETlhfeUPbi5IMTUZejMbPfZSUhIQHJyMoKCgqQ2tVqNgIAAxMXFAQDi4uLg7OwsBR0ACAoKgoWFBU6cOFHkvLOzs5GRkaF1IyIyR+b4Uw1RWWO259lJTk4GALi5uWm1u7m5SdOSk5Ph6uqqNd3KygouLi5Sn8LMnz8f4eHhBq6YiMwFwwEVR/52U5ZGLEg3ZjuyY0xhYWFIT0+XbklJSaW2bH4IU3nBEQkiMhdmG3bc3d0BACkpKVrtKSkp0jR3d3ekpqZqTX/27Bnu378v9SmMjY0NVCqV1o2IyFQYComMy2zDjo+PD9zd3REdHS21ZWRk4MSJE9BoNAAAjUaDtLQ0nD59Wurz448/Ii8vDwEBAaVeMxERyRNHKss2k+6zk5mZid9//126n5CQgPj4eLi4uMDT0xNjx47FnDlzUKtWLfj4+GDatGnw8PBAjx49AAB169ZF586dMXz4cKxZswZPnz7F6NGj0adPHx6JRfQcfkgbx/PPK59jIvNl0rBz6tQpvPHGG9L98ePHAwAGDhyIjRs3YtKkScjKysKIESOQlpaG1q1bY//+/bC1tZUes3XrVowePRqBgYGwsLBAr169sGLFilJfFyIiIjJPJg077du3hxCiyOkKhQKzZs3CrFmziuzj4uKCbdu2GaM8s8S/HklOXtyeeRQMERmD2e6zQ+aHv1kTEVFZxLBDREREssawQ0RkIhwtpZIwh23HHGrQBcMOERERyRrDDpGJlZW/jIiIyiqGHSIiIpI1s70QKOmvqIvYeU/ZY5aH9HJEg/TBizSSsRTns6i0tkd+ThoGR3aIyiFDf4DyA5mIzBnDDhWKX15lF187IjIFcz66kGGHiMyGuX5QElHZxrBDL1UWvnzM+a8JMl/cbojKD4YdIqJXKO/BqDyvO8kDww5RGcAvm7KLrx29iNtE6WPYISIiKiEGGPPG8+wQAL5RiYhIvjiyY8YYQOhlyvt+JFR2mOO2Wpr1mNu6l0cMO0REVGzmGGQMjcGo7OPPWGQQfIOSOTPXS6aYE32eI2O833WZpzlcMoSfdWUTR3aIZEKOH8JyXCciKn0c2aFX4hcOUfliDiMoRIbEkR0iKlUMz0RU2jiyQ0RUBAYzInngyI5M8GiBohmz3vJwJAoRUVnHkR0iIqIyjn90vRxHdqjYTPHm4huaiIj0xbBDREREssafsYiIiAyII9DmhyM7VCbxw8R4yutO1+VxnYnKC47sEJFZYwgxHT73JBcMO+VQefwAK4/rTOaF2yCfAzId/oxFRGaHX4plF18781ZeXx+GHRPhie6IzBvfQyVX1j+LynLtpI1hh6iMKetfIFQ0c39dzb0+oqIw7JQj/KAiKojhkV7E7UF+uIMyEZUp/CIiIn1xZIeISE8MXERlC8MOERFRMTH4lg38GYuIiMoUBgzSF0d2iIiIyGjMIZwy7BAREZHBmUPIycefsUzInDYEIiIiueLIDhEREcmabMJOREQEvL29YWtri4CAAPz666+mLomIiIjMgCzCzpdffonx48djxowZOHPmDBo1aoTg4GCkpqaaujQiIjIy7hJAryKLsLNkyRIMHz4cgwcPhp+fH9asWQN7e3ts2LDB1KURERmdLpe84GUxqDwr8zso5+Tk4PTp0wgLC5PaLCwsEBQUhLi4OBNWRkRUfMYOJt5T9uDmgpBSXSaRqZT5sPP3338jNzcXbm5uWu1ubm64cuVKoY/Jzs5Gdna2dD89PR0AkJGRYfD68rIfSf/PyMhAXvYj6d+X0aWvoednymVzfiWbn5zWpbzNT9e+nuN24LfwYKlP/RkHCu3z/PzqzzggPaY01qW8zU9O62Ls+Rnj+zV/OQAghHh5R1HG3bp1SwAQx44d02qfOHGieP311wt9zIwZMwQA3njjjTfeeONNBrekpKSXZoUyv89OpUqVYGlpiZSUFK32lJQUuLu7F/qYsLAwpKenS7cHDx7gxo0bSEtL02o3xC0pKQkAcOnSJa1/C2srTh9Tzk9O61Le5iendSlv85PTupS3+clpXfSdX1JSksG/X9PT05GWloakpCR4eHjgZcr8z1hKpRL+/v6Ijo5Gjx49AAB5eXmIjo7G6NGjC32MjY0NbGxstNqcnZ2NWqeTk5PWv4W1FaePKecnp3Upb/OT07qUt/nJaV3K2/zktC76zk+lUkGlUsEY1Gr1K/uU+bADAOPHj8fAgQPRrFkzvP7661i2bBmysrIwePBgU5dGREREJiaLsNO7d2/cvXsX06dPR3JyMho3boz9+/cX2GmZiIiIyh9ZhB0AGD16dJE/W5mSjY0NZsyYAZVKJf373//+FwC02orTx5Tzk9O6lLf5yWldytv85LQu5W1+clqX4szvxV1HSptCiFcdr0VERERUdpX5o7GIiIiIXoZhh4iIiGSNYYeIiIhkjWGHiIiIZI1hh4iIiGSNYYeIiIhkjWGHiIiIZI1hh4iIiGSNYYeIzNLNmzehUCgQHx8PADhy5AgUCgXS0tJMUk/79u0xduxYkyybiEqGYYeIDO7u3bsYNWoUPD09YWNjA3d3dwQHB+OXX34p9jxbtmyJO3fuSFc43rhxI5ydnV/5OF37EZF8yebaWERkPnr16oWcnBxs2rQJ1atXR0pKCqKjo3Hv3r1iz1OpVMLd3d2AVRJRecGRHSIyqLS0NPz000/45JNP8MYbb8DLywuvv/46wsLC0L17d6mfQqHA6tWr0aVLF9jZ2aF69er4+uuvi5zv8z9jHTlyBIMHD0Z6ejoUCgUUCgVmzpypU30zZ85E48aNsWXLFnh7e0OtVqNPnz54+PCh1CcrKwsDBgyAo6MjqlSpgsWLFxeYT3Z2NiZMmIDXXnsNDg4OCAgIwJEjRwAAT548Qb169TBixAip/40bN+Dk5IQNGzboVCcRGQ7DDhEZlKOjIxwdHbFr1y5kZ2e/tO+0adPQq1cvnDt3Dn379kWfPn1w+fLlVy6jZcuWWLZsGVQqFe7cuYM7d+5gwoQJOtd448YN7Nq1C7t378bu3bsRGxuLBQsWSNMnTpyI2NhYfPfddzh48CCOHDmCM2fOaM1j9OjRiIuLw/bt23H+/Hm888476Ny5M65fvw5bW1ts3boVmzZtwnfffYfc3Fz069cPHTt2xJAhQ3Suk4gMRBARGdjXX38tKlSoIGxtbUXLli1FWFiYOHfunFYfAGLkyJFabQEBAWLUqFFCCCESEhIEAHH27FkhhBAxMTECgHjw4IEQQoioqCihVqtfWcuL/WbMmCHs7e1FRkaG1DZx4kQREBAghBDi4cOHQqlUiq+++kqafu/ePWFnZyf+85//CCGE+PPPP4WlpaW4deuW1rICAwNFWFiYdH/hwoWiUqVKYvTo0aJKlSri77//fmW9RGR4HNkhIoPr1asXbt++je+//x6dO3fGkSNH0LRpU2zcuFGrn0ajKXBfl5GdkvL29oaTk5N0v0qVKkhNTQXwz6hPTk4OAgICpOkuLi6oU6eOdP/ChQvIzc1F7dq1pZEsR0dHxMbG4saNG1K/jz76CLVr18aqVauwYcMGVKxY0ejrRkQFcQdlIjIKW1tbdOzYER07dsS0adMwbNgwzJgxA4MGDTJ1abC2tta6r1AokJeXp/PjMzMzYWlpidOnT8PS0lJrmqOjo/T/1NRUXLt2DZaWlrh+/To6d+5cssKJqFg4skNEpcLPzw9ZWVlabcePHy9wv27dujrNT6lUIjc312D15atRowasra1x4sQJqe3Bgwe4du2adL9JkybIzc1FamoqatasqXV7/oixIUOGoEGDBti0aRMmT55cKqNWRFQQR3aIyKDu3buHd955B0OGDEHDhg3h5OSEU6dOYeHChXjzzTe1+u7YsQPNmjVD69atsXXrVvz6669Yv369Tsvx9vZGZmYmoqOj0ahRI9jb28Pe3r7E9Ts6OmLo0KGYOHEiKlasCFdXV/z3v/+FhcX//jasXbs2+vbtiwEDBmDx4sVo0qQJ7t69i+joaDRs2BAhISGIiIhAXFwczp8/j2rVqmHPnj3o27cvjh8/DqVSWeI6iUh3HNkhIoNydHREQEAAli5dirZt26J+/fqYNm0ahg8fjlWrVmn1DQ8Px/bt29GwYUNs3rwZX3zxBfz8/HRaTsuWLTFy5Ej07t0blStXxsKFCw22Dp9++inatGmDbt26ISgoCK1bt4a/v79Wn6ioKAwYMAAfffQR6tSpgx49euDkyZPw9PTElStXMHHiRERGRqJatWoAgMjISPz999+YNm2aweokIt0ohBDC1EUQUfmjUCiwc+dO9OjRw9SlEJHMcWSHiIiIZI1hh4iIiGSNOygTkUnwF3QiKi0c2SEiIiJZY9ghIiIiWWPYISIiIllj2CEiIiJZY9ghIiIiWWPYISIiIllj2CEiIiJZY9ghIiIiWWPYISIiIln7f60X/kEmmufrAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "split_lengths = [num_of_tokens(split.page_content) for split in semantic_splits]\n",
    "\n",
    "# Create a bar graph\n",
    "plt.bar(range(len(split_lengths)), split_lengths)\n",
    "plt.xlabel(\"Split Index\")\n",
    "plt.ylabel(\"Split Content Length\")\n",
    "plt.title(\"Semantic Chunker Split Page Content Lengths\")\n",
    "plt.xticks(range(len(split_lengths)), [])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChromaParallel Class: Parallel Document Embedding\n",
    "The ChromaParallel class is an extension of the Chroma class to enable parallel processing of document embedding and storage using multiple worker processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chroma is an AI-native open source vector database designed to enhance developer productivity and satisfaction. It is licensed under the Apache 2.0 license. \n",
    "\n",
    "- <b> Generate Vectorspace </b> : The `from_documents` class method creates a vector store from a list of documents.\n",
    "\n",
    "##### Reference\n",
    "\n",
    "* [Chroma LangChain Documentation](https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/)\n",
    "* [Chroma Official Documentation](https://docs.trychroma.com/getting-started)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "class ChromaParallel(Chroma):\n",
    "\n",
    "    async def afrom_documents(documents, embedding, num_workers=2):\n",
    "        db = Chroma(embedding_function=embedding)\n",
    "        # create list of num_workers empty lists\n",
    "        doc_groups = [[] for _ in range(num_workers)]\n",
    "\n",
    "        for i in range(len(documents)):\n",
    "            doc_groups[i % num_workers].append(documents[i])\n",
    "\n",
    "        tasks = [db.aadd_documents(group) for group in doc_groups]\n",
    "        await asyncio.gather(*tasks)\n",
    "        return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='<h1 id='2' style='font-size:22px'>Classifying Software Changes:<br>Clean or Buggy?</h1><br><p id='3' data-category='paragraph' style='font-size:20px'>Sunghun Kim, E. James Whitehead Jr., Member, IEEE, and Yi Zhang, Member, IEEE</p><p id='4' data-category='paragraph' style='font-size:16px'>Abstract-This paper introduces a new technique for predicting latent software bugs, called change classification. Change<br>classification uses a machine learning classifier to determine whether a new software change is more similar to prior buggy changes or<br>clean changes. In this manner, change classification predicts the existence of bugs in software changes. The classifier is trained using<br>features (in the machine learning sense) extracted from the revision history of a software project stored in its software configuration<br>management repository. The trained classifier can classify changes as buggy or clean, with a 78 percent accuracy and a 60 percent<br>buggy change recall on average.' metadata={'total_pages': 16}\n",
      "Wall time: 15.64 sec\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import time\n",
    "\n",
    "now = time.time()\n",
    "\n",
    "# 3. Embed & indexing\n",
    "loop = asyncio.get_event_loop()\n",
    "semantic_vectorstore = await ChromaParallel.afrom_documents(\n",
    "    documents=semantic_splits,\n",
    "    embedding=UpstageEmbeddings(model=\"solar-embedding-1-large\"),\n",
    "    num_workers=3,\n",
    ")\n",
    "semantic_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# 4. retrive\n",
    "result_docs = semantic_retriever.invoke(\"What is Bug Classification?\")\n",
    "print(result_docs[1])\n",
    "print(f\"Wall time: {time.time() - now:.2f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BaseModel.__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is bug classification? How it works?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m result_docs \u001b[38;5;241m=\u001b[39m semantic_retriever\u001b[38;5;241m.\u001b[39minvoke(query)\n\u001b[0;32m----> 5\u001b[0m gc_result \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhistory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(gc_result)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/langchain_core/runnables/base.py:3024\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3023\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3024\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3025\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3026\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/langchain_core/runnables/base.py:4713\u001b[0m, in \u001b[0;36mRunnableLambda.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4699\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke this Runnable synchronously.\u001b[39;00m\n\u001b[1;32m   4700\u001b[0m \n\u001b[1;32m   4701\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4710\u001b[0m \u001b[38;5;124;03m    TypeError: If the Runnable is a coroutine function.\u001b[39;00m\n\u001b[1;32m   4711\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4712\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunc\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 4713\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4714\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4715\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4716\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4717\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4718\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4719\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4720\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   4721\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke a coroutine function synchronously.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4722\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse `ainvoke` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4723\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/langchain_core/runnables/base.py:1927\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, serialized, **kwargs)\u001b[0m\n\u001b[1;32m   1923\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   1924\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[1;32m   1925\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   1926\u001b[0m         Output,\n\u001b[0;32m-> 1927\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1928\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1929\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1930\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1931\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1932\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1933\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1934\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1935\u001b[0m     )\n\u001b[1;32m   1936\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1937\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/langchain_core/runnables/config.py:396\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    395\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/langchain_core/runnables/base.py:4567\u001b[0m, in \u001b[0;36mRunnableLambda._invoke\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   4565\u001b[0m                 output \u001b[38;5;241m=\u001b[39m chunk\n\u001b[1;32m   4566\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4567\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4568\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   4569\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4570\u001b[0m \u001b[38;5;66;03m# If the output is a Runnable, invoke it\u001b[39;00m\n\u001b[1;32m   4571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/langchain_core/runnables/config.py:396\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    395\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/langchain_core/load/serializable.py:125\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: BaseModel.__init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "# Finally query using RAG\n",
    "query = \"What is bug classification? How it works?\"\n",
    "result_docs = semantic_retriever.invoke(query)\n",
    "\n",
    "gc_result = chain.invoke({\"history\": history, \"context\": result_docs, \"input\": query})\n",
    "print(gc_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying bugs is beneficial for several reasons:\n",
      "\n",
      "1. **Prioritization**: By categorizing bugs based on their severity, impact, and other factors, development teams can prioritize their work and focus on the most critical issues first. This helps to ensure that the most pressing problems are addressed promptly, improving the overall quality of the software.\n",
      "\n",
      "2. **Efficiency**: Classifying bugs allows teams to manage their workload more effectively. They can assign bugs to specific team members based on their expertise or availability, streamlining the bug fixing process.\n",
      "\n",
      "3. **Transparency**: A well-defined bug classification system provides transparency into the bug tracking process. This enables stakeholders, such as project managers, to understand the current state of the software and make informed decisions about resource allocation and project timelines.\n",
      "\n",
      "4. **Improved Communication**: Clear bug classification categories facilitate better communication within the development team and between the team and other stakeholders. Everyone involved in the project can understand the nature and severity of the bugs, making it easier to coordinate efforts and track progress.\n",
      "\n",
      "5. **Quality Assurance**: Bug classification helps in identifying patterns and trends in the types of bugs that occur in the software. This information can be used to improve the development process, prevent similar bugs from occurring in the future, and enhance the overall quality of the software.\n",
      "\n",
      "6. **Customer Satisfaction**: By effectively managing and resolving bugs, development teams can improve the user experience and increase customer satisfaction. This can lead to better product adoption, positive reviews, and increased customer loyalty.\n",
      "\n",
      "In summary, bug classification is a crucial part of the software development process that helps teams to manage their workload, improve communication, and ensure that the software is of high quality and meets user expectations.\n"
     ]
    }
   ],
   "source": [
    "history = [HumanMessage(query), AIMessage(gc_result)]\n",
    "\n",
    "query = \"Why it is good?\"\n",
    "result_docs = semantic_retriever.invoke(query)\n",
    "\n",
    "gc_result = chain.invoke({\"history\": history, \"context\": result_docs, \"input\": query})\n",
    "print(gc_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For an in-depth look at the different types of RAG, please refer to the files '09. Smart RAG' and '10. Tool_RAG'.\n",
    "\n",
    "- [09. Smart RAG.ipynb](https://github.com/UpstageAI/cookbook/blob/main/cookbooks/upstage/Solar-Full-Stack-LLM-101/09_Smart_RAG.ipynb)\n",
    "- [10. Tool_RAG.ipynb](https://github.com/UpstageAI/cookbook/blob/main/cookbooks/upstage/Solar-Full-Stack-LLM-101/10_tool_RAG.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Session 5] Gradio\n",
    "\n",
    "<b> Comprehensive RAG System for PDFs </b> : Use Gradio and RAG techniques to process PDF documents and generate real-time, interactive responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU gradio python-dotenv langchain-upstage python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "from langchain_upstage import (\n",
    "    ChatUpstage,\n",
    "    UpstageEmbeddings,\n",
    "    UpstageLayoutAnalysisLoader,\n",
    ")\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema import AIMessage, HumanMessage\n",
    "from langchain_text_splitters import (\n",
    "    Language,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "llm = ChatUpstage(streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More general chat\n",
    "chat_with_history_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{message}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = chat_with_history_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    history_langchain_format = []\n",
    "    for human, ai in history:\n",
    "        history_langchain_format.append(HumanMessage(content=human))\n",
    "        history_langchain_format.append(AIMessage(content=ai))\n",
    "\n",
    "    return chain.invoke({\"message\": message, \"history\": history_langchain_format})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.ChatInterface(\n",
    "        chat,\n",
    "        examples=[\n",
    "            \"How to eat healthy?\",\n",
    "            \"Best Places in Korea\",\n",
    "            \"How to make a chatbot?\",\n",
    "        ],\n",
    "        title=\"Solar Chatbot\",\n",
    "        description=\"Upstage Solar Chatbot\",\n",
    "    )\n",
    "    chatbot.chatbot.height = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradio_ChatPDF\n",
    "\n",
    "- <b> Comprehensive RAG System for PDFs </b> : Use Gradio and RAG techniques to process PDF documents and generate real-time, interactive responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history, retriever):\n",
    "    result_docs = \"\"\n",
    "    if retriever:\n",
    "        result_docs = retriever.invoke(message)\n",
    "\n",
    "    history_langchain_format = []\n",
    "    for human, ai in history:\n",
    "        history_langchain_format.append(HumanMessage(content=human))\n",
    "        history_langchain_format.append(AIMessage(content=ai))\n",
    "\n",
    "    generator = chain.stream(\n",
    "        {\n",
    "            \"context\": result_docs,\n",
    "            \"message\": message,\n",
    "            \"history\": history_langchain_format,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    assistant = \"\"\n",
    "    for gen in generator:\n",
    "        assistant += gen\n",
    "        yield assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_upload(file):\n",
    "    layzer = UpstageLayoutAnalysisLoader(file, output_type=\"html\", use_ocr=False)\n",
    "    docs = layzer.load()\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "    print(len(splits))\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=splits,\n",
    "        embedding=UpstageEmbeddings(\n",
    "            model=\"solar-embedding-1-large\", embed_batch_size=100\n",
    "        ),\n",
    "    )\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "    return file, retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Solar Chatbot\")\n",
    "    gr.Markdown(\n",
    "        \"Upstage Solar Chatbot\",\n",
    "    )\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            file = gr.File()\n",
    "            retreiver = gr.State()\n",
    "        with gr.Column():\n",
    "            chatbot = gr.ChatInterface(\n",
    "                chat,\n",
    "                examples=[\n",
    "                    [\"How to eat healthy?\"],\n",
    "                    [\"Best Places in Korea\"],\n",
    "                    [\"How to make a chatbot?\"],\n",
    "                ],\n",
    "                additional_inputs=retreiver,\n",
    "            )\n",
    "    chatbot.chatbot.height = 300\n",
    "    file.upload(file_upload, file, [file, retreiver])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Building Your Own AI-Powered Chatbot! 🤖\n",
    "\n",
    "\n",
    "Congratulations on completing the course on building chatbots using Language Models (LLMs), Layout Analysis (LA), custom tools, and Groundedness Checks (GC)! Now, showcase your brilliant ideas by participating in a hackathon and leveraging the Solar API! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
